{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load the Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_name = \"income_evaluation.csv\"\n",
    "data_path = os.path.join(\"..\",\"data\",\"raw\",file_name)\n",
    "data = pd.read_csv(data_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass   fnlwgt   education   education-num  \\\n",
       "0   39          State-gov    77516   Bachelors              13   \n",
       "1   50   Self-emp-not-inc    83311   Bachelors              13   \n",
       "2   38            Private   215646     HS-grad               9   \n",
       "3   53            Private   234721        11th               7   \n",
       "4   28            Private   338409   Bachelors              13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "    capital-gain   capital-loss   hours-per-week  native-country  income  \n",
       "0           2174              0               40   United-States   <=50K  \n",
       "1              0              0               13   United-States   <=50K  \n",
       "2              0              0               40   United-States   <=50K  \n",
       "3              0              0               40   United-States   <=50K  \n",
       "4              0              0               40            Cuba   <=50K  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 int64\n",
       " workclass         object\n",
       " fnlwgt             int64\n",
       " education         object\n",
       " education-num      int64\n",
       " marital-status    object\n",
       " occupation        object\n",
       " relationship      object\n",
       " race              object\n",
       " sex               object\n",
       " capital-gain       int64\n",
       " capital-loss       int64\n",
       " hours-per-week     int64\n",
       " native-country    object\n",
       " income            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (32561, 15)\n",
      "cleaned data shape:  (32561, 15)\n",
      "X shape (32561, 14)\n",
      " <=50K    24720\n",
      " >50K      7841\n",
      "Name:  income, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   fnlwgt   education-num   capital-gain   capital-loss   hours-per-week\n",
       "0   39    77516              13           2174              0               40\n",
       "1   50    83311              13              0              0               13\n",
       "2   38   215646               9              0              0               40\n",
       "3   53   234721               7              0              0               40\n",
       "4   28   338409              13              0              0               40"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col = \" income\"\n",
    "print(\"data shape: \", data.shape)\n",
    "data_cleaned = data.dropna().copy()\n",
    "print(\"cleaned data shape: \", data_cleaned.shape)\n",
    "X, y = data_cleaned.drop(columns=[label_col]), data_cleaned[label_col]\n",
    "print(\"X shape\", X.dropna().shape)\n",
    "print(y.value_counts())\n",
    "X_num = X.loc[:, X.dtypes == np.int64].dropna()\n",
    "X_num.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt   education-num   capital-gain  \\\n",
       "count  32561.000000  3.256100e+04    32561.000000   32561.000000   \n",
       "mean      38.581647  1.897784e+05       10.080679    1077.648844   \n",
       "std       13.640433  1.055500e+05        2.572720    7385.292085   \n",
       "min       17.000000  1.228500e+04        1.000000       0.000000   \n",
       "25%       28.000000  1.178270e+05        9.000000       0.000000   \n",
       "50%       37.000000  1.783560e+05       10.000000       0.000000   \n",
       "75%       48.000000  2.370510e+05       12.000000       0.000000   \n",
       "max       90.000000  1.484705e+06       16.000000   99999.000000   \n",
       "\n",
       "        capital-loss   hours-per-week  \n",
       "count   32561.000000     32561.000000  \n",
       "mean       87.303830        40.437456  \n",
       "std       402.960219        12.347429  \n",
       "min         0.000000         1.000000  \n",
       "25%         0.000000        40.000000  \n",
       "50%         0.000000        40.000000  \n",
       "75%         0.000000        45.000000  \n",
       "max      4356.000000        99.000000  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "label_col = data.shape[1]-1\n",
    "X_sonar = X.values.astype(float)\n",
    "y_sonar = y.values\n",
    "y_sonar = column_or_1d(y_sonar, warn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23765000e+18, 1.83531326e+02, 8.96930300e-02, ...,\n",
       "        3.30600000e+03, 5.49220000e+04, 4.91000000e+02],\n",
       "       [1.23765000e+18, 1.83598371e+02, 1.35285032e-01, ...,\n",
       "        3.23000000e+02, 5.16150000e+04, 5.41000000e+02],\n",
       "       [1.23765000e+18, 1.83680207e+02, 1.26185092e-01, ...,\n",
       "        2.87000000e+02, 5.20230000e+04, 5.13000000e+02],\n",
       "       ...,\n",
       "       [1.23765000e+18, 1.31552562e+02, 5.16669864e+01, ...,\n",
       "        7.30300000e+03, 5.70130000e+04, 6.22000000e+02],\n",
       "       [1.23765000e+18, 1.31477151e+02, 5.17530678e+01, ...,\n",
       "        4.47000000e+02, 5.18770000e+04, 2.29000000e+02],\n",
       "       [1.23765000e+18, 1.31665012e+02, 5.18053075e+01, ...,\n",
       "        4.47000000e+02, 5.18770000e+04, 2.33000000e+02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sonar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Split Data For Cross Validation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split the data into four new datasets, training features, training outcome, test features, \n",
    "# and test outcome. Set the size of the test data to be 20% of the full dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sonar, y_sonar, test_size=0.20, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.796500 (0.013167)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.918750 (0.009953)\n",
      "KNN: 0.782750 (0.012696)\n",
      "CART: 0.985125 (0.003281)\n",
      "NB: 0.796250 (0.012784)\n",
      "SVM: 0.533500 (0.014820)\n",
      "RF: 0.933000 (0.005948)\n"
     ]
    }
   ],
   "source": [
    "num_folds=10\n",
    "scoring='accuracy'\n",
    "models = []\n",
    "models.append(('LR',  LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART',DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('NB',  GaussianNB()))\n",
    "models.append(('SVM', SVC(random_state=seed)))\n",
    "models.append(('RF',  RandomForestClassifier(max_depth=3, random_state=seed)))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Standardization Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: 0.955250 (0.006119)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLDA: 0.911500 (0.008174)\n",
      "ScaledKNN: 0.904250 (0.006736)\n",
      "ScaledCART: 0.984750 (0.002894)\n",
      "ScaledNB: 0.942000 (0.006524)\n",
      "ScaledSVM: 0.954500 (0.005368)\n",
      "ScaledRF: 0.933000 (0.005948)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART',Pipeline([('Scaler',StandardScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('ScaledNB',Pipeline([('Scaler',StandardScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('ScaledRF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training a  SVM classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No normalization or standartization\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "83.30%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "54.40%\n",
      "\n",
      "##################################################\n",
      "Data after standard scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "97.22%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "96.65%\n",
      "\n",
      "##################################################\n",
      "Data after min-max scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "86.91%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "86.55%\n",
      "\n",
      "##################################################\n",
      "Data after max-abs scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "86.02%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "85.80%\n",
      "\n",
      "##################################################\n",
      "Data after robust scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "98.52%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.35%\n",
      "\n",
      "##################################################\n",
      "Data after power transformation (Yeo-Johnson)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction accuracy for the training dataset\n",
      "98.78%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.55%\n",
      "\n",
      "##################################################\n",
      "Data after quantile transformation (gaussian pdf)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "98.58%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.10%\n",
      "\n",
      "##################################################\n",
      "Data after quantile transformation (uniform pdf)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "97.82%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "97.65%\n",
      "\n",
      "##################################################\n",
      "Data after sample-wise L2 normalizing\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "79.65%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "79.80%\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# std_scale = StandardScaler().fit(X_train)\n",
    "distributions = [\n",
    "    ('Data after standard scaling',\n",
    "        StandardScaler()),\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler()),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler()),\n",
    "    ('Data after robust scaling',\n",
    "        RobustScaler(quantile_range=(25, 75))),\n",
    "    ('Data after power transformation (Yeo-Johnson)',\n",
    "     PowerTransformer(method='yeo-johnson')),\n",
    "#     ('Data after power transformation (Box-Cox)',\n",
    "#      PowerTransformer(method='box-cox')),\n",
    "    ('Data after quantile transformation (gaussian pdf)',\n",
    "        QuantileTransformer(output_distribution='normal')\n",
    "        ),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')\n",
    "        ),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer()),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(\"No normalization or standartization\")\n",
    "svc_scaled = SVC(C=1.5, random_state=seed)\n",
    "fit_std = svc_scaled.fit(X_train, y_train)\n",
    "pred_train_std = svc_scaled.predict(X_train)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "\n",
    "pred_test_std = svc_scaled.predict(X_test)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "\n",
    "\n",
    "print(\"#\"*50)\n",
    "\n",
    "    \n",
    "for name, dist in distributions:\n",
    "    print(name)\n",
    "    std_scale = dist.fit(X_train)\n",
    "    X_train_std = std_scale.transform(X_train)\n",
    "    X_test_std = std_scale.transform(X_test)\n",
    "\n",
    "    # on standardized data\n",
    "    svc_scaled = SVC(C=1.5, random_state=seed)\n",
    "    fit_std = svc_scaled.fit(X_train_std, y_train)\n",
    "    pred_train_std = svc_scaled.predict(X_train_std)\n",
    "\n",
    "    print('\\nPrediction accuracy for the training dataset')\n",
    "    print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "\n",
    "    pred_test_std = svc_scaled.predict(X_test_std)\n",
    "\n",
    "    print('\\nPrediction accuracy for the test dataset')\n",
    "    print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "    \n",
    "    \n",
    "    print(\"#\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(names, resutls, test_scores):\n",
    "    print()\n",
    "    print(\"#\"*30 +\"Results\" + \"#\"*30)\n",
    "    counter = 0\n",
    "    class color:\n",
    "       PURPLE = '\\033[95m'\n",
    "       CYAN = '\\033[96m'\n",
    "       DARKCYAN = '\\033[36m'\n",
    "       BLUE = '\\033[94m'\n",
    "       GREEN = '\\033[92m'\n",
    "       YELLOW = '\\033[93m'\n",
    "       RED = '\\033[91m'\n",
    "       BOLD = '\\033[1m'\n",
    "       UNDERLINE = '\\033[4m'\n",
    "       END = '\\033[0m'\n",
    "\n",
    "\n",
    "    # Get max row\n",
    "    clf_names = set([name.split(\"_\")[1] for name in names])\n",
    "    max_mean = {name:0 for name in clf_names}\n",
    "    max_mean_counter = {name:0 for name in clf_names}\n",
    "    for name,result in zip(names,results):\n",
    "        counter +=1\n",
    "        clf_name = name.split(\"_\")[1]\n",
    "        if result.mean()>max_mean[clf_name]:\n",
    "            max_mean_counter[clf_name] = counter\n",
    "            max_mean[clf_name] = result.mean()\n",
    "\n",
    "    # print max row in BOLD\n",
    "    counter = 0\n",
    "    prev_clf_name = names[0].split(\"_\")[1]\n",
    "    for name,result ,score in zip(names,results,test_scores): \n",
    "        counter +=1\n",
    "        clf_name = name.split(\"_\")[1]\n",
    "        if prev_clf_name != clf_name:\n",
    "            print()\n",
    "            prev_clf_name = clf_name\n",
    "        msg = \"%s: %f (%f) [test_score:%.3f]\" % (name, result.mean(), result.std(), score)\n",
    "        if counter==max_mean_counter[clf_name]:\n",
    "            print(color.BOLD + msg)\n",
    "        else:\n",
    "            print(color.END + msg)\n",
    "            \n",
    "def print_results2(names, results_mean,results_std, test_scores):\n",
    "    print()\n",
    "    print(\"#\"*30 +\"Results\" + \"#\"*30)\n",
    "    class color:\n",
    "       PURPLE = '\\033[95m'\n",
    "       CYAN = '\\033[96m'\n",
    "       DARKCYAN = '\\033[36m'\n",
    "       BLUE = '\\033[94m'\n",
    "       GREEN = '\\033[92m'\n",
    "       YELLOW = '\\033[93m'\n",
    "       RED = '\\033[91m'\n",
    "       BOLD = '\\033[1m'\n",
    "       UNDERLINE = '\\033[4m'\n",
    "       END = '\\033[0m'\n",
    "\n",
    "\n",
    "\n",
    "    # print max row in BOLD\n",
    "    prev_clf_name = names[0].split(\"_\")[1]\n",
    "    for name,mean,std, score in zip(names,results_mean,results_std, test_scores): \n",
    "        clf_name = name.split(\"_\")[1]\n",
    "        if prev_clf_name != clf_name:\n",
    "            print()\n",
    "            prev_clf_name = clf_name\n",
    "        \n",
    "        msg = \"%s: %f (%f) [test_score:%.3f]\" % (name, mean, std, score)\n",
    "        if mean==max(results_mean):\n",
    "            print(color.BOLD + msg)\n",
    "        else:\n",
    "            print(color.END + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LR: 0.796500 (0.013167)\n",
      "Scaled_LR: 0.955250 (0.006119)\n",
      "MinMax_LR: 0.895125 (0.009592)\n",
      "MaxAbsScaler_LR: 0.874750 (0.008958)\n",
      "RobustScaler_LR: 0.974750 (0.005911)\n",
      "QuantileTransformer-Normal_LR: 0.971000 (0.005148)\n",
      "QuantileTransformer-Uniform_LR: 0.954625 (0.004332)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_LR: 0.979375 (0.004945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LR: 0.797875 (0.012451)\n",
      "_LR-PCA: 0.796500 (0.013167)\n",
      "Scaled_LR-PCA: 0.847500 (0.010969)\n",
      "MinMax_LR-PCA: 0.796875 (0.013029)\n",
      "MaxAbsScaler_LR-PCA: 0.797125 (0.012637)\n",
      "RobustScaler_LR-PCA: 0.957250 (0.007842)\n",
      "QuantileTransformer-Normal_LR-PCA: 0.821875 (0.013326)\n",
      "QuantileTransformer-Uniform_LR-PCA: 0.821000 (0.012597)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_LR-PCA: 0.625375 (0.018346)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LR-PCA: 0.797875 (0.012451)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LDA: 0.918750 (0.009953)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LDA: 0.911500 (0.008174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LDA: 0.911500 (0.008174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LDA: 0.911500 (0.008174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LDA: 0.911500 (0.008174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_LDA: 0.969750 (0.004323)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_LDA: 0.967875 (0.006022)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_LDA: 0.962750 (0.005777)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LDA: 0.905250 (0.008511)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LDA-PCA: 0.918750 (0.009953)\n",
      "Scaled_LDA-PCA: 0.837625 (0.012911)\n",
      "MinMax_LDA-PCA: 0.790875 (0.013954)\n",
      "MaxAbsScaler_LDA-PCA: 0.777500 (0.012222)\n",
      "RobustScaler_LDA-PCA: 0.852750 (0.010706)\n",
      "QuantileTransformer-Normal_LDA-PCA: 0.824625 (0.015603)\n",
      "QuantileTransformer-Uniform_LDA-PCA: 0.812875 (0.012699)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_LDA-PCA: 0.623625 (0.018911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LDA-PCA: 0.797875 (0.012451)\n",
      "_KNN: 0.782750 (0.012696)\n",
      "Scaled_KNN: 0.904250 (0.006736)\n",
      "MinMax_KNN: 0.877375 (0.009754)\n",
      "MaxAbsScaler_KNN: 0.868625 (0.009754)\n",
      "RobustScaler_KNN: 0.952625 (0.005137)\n",
      "QuantileTransformer-Normal_KNN: 0.943875 (0.009261)\n",
      "QuantileTransformer-Uniform_KNN: 0.955250 (0.006169)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_KNN: 0.955875 (0.004509)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_KNN: 0.782500 (0.014098)\n",
      "_KNN-PCA: 0.782750 (0.012696)\n",
      "Scaled_KNN-PCA: 0.845750 (0.008700)\n",
      "MinMax_KNN-PCA: 0.804875 (0.013422)\n",
      "MaxAbsScaler_KNN-PCA: 0.778500 (0.014435)\n",
      "RobustScaler_KNN-PCA: 0.970375 (0.006276)\n",
      "QuantileTransformer-Normal_KNN-PCA: 0.860500 (0.008771)\n",
      "QuantileTransformer-Uniform_KNN-PCA: 0.866500 (0.011576)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_KNN-PCA: 0.822500 (0.013062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_KNN-PCA: 0.782125 (0.013751)\n",
      "_CART: 0.985125 (0.003281)\n",
      "Scaled_CART: 0.984750 (0.002894)\n",
      "MinMax_CART: 0.984750 (0.002947)\n",
      "MaxAbsScaler_CART: 0.984750 (0.002947)\n",
      "RobustScaler_CART: 0.984750 (0.002894)\n",
      "QuantileTransformer-Normal_CART: 0.985125 (0.003034)\n",
      "QuantileTransformer-Uniform_CART: 0.985000 (0.003211)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_CART: 0.984375 (0.003802)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_CART: 0.719125 (0.015613)\n",
      "_CART-PCA: 0.985125 (0.003281)\n",
      "Scaled_CART-PCA: 0.803750 (0.020210)\n",
      "MinMax_CART-PCA: 0.746250 (0.016441)\n",
      "MaxAbsScaler_CART-PCA: 0.718375 (0.020326)\n",
      "RobustScaler_CART-PCA: 0.971000 (0.003945)\n",
      "QuantileTransformer-Normal_CART-PCA: 0.818250 (0.014771)\n",
      "QuantileTransformer-Uniform_CART-PCA: 0.818000 (0.015484)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_CART-PCA: 0.803125 (0.009392)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_CART-PCA: 0.718375 (0.015125)\n",
      "_NB: 0.796250 (0.012784)\n",
      "Scaled_NB: 0.942000 (0.006524)\n",
      "MinMax_NB: 0.941750 (0.006643)\n",
      "MaxAbsScaler_NB: 0.941750 (0.006643)\n",
      "RobustScaler_NB: 0.942000 (0.006524)\n",
      "QuantileTransformer-Normal_NB: 0.924625 (0.011097)\n",
      "QuantileTransformer-Uniform_NB: 0.924125 (0.012210)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_NB: 0.985125 (0.002820)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_NB: 0.797875 (0.012451)\n",
      "_NB-PCA: 0.796250 (0.012784)\n",
      "Scaled_NB-PCA: 0.840500 (0.013877)\n",
      "MinMax_NB-PCA: 0.781625 (0.014959)\n",
      "MaxAbsScaler_NB-PCA: 0.768125 (0.012957)\n",
      "RobustScaler_NB-PCA: 0.880000 (0.011713)\n",
      "QuantileTransformer-Normal_NB-PCA: 0.835375 (0.013452)\n",
      "QuantileTransformer-Uniform_NB-PCA: 0.826375 (0.013115)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_NB-PCA: 0.628500 (0.016954)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_NB-PCA: 0.794250 (0.013707)\n",
      "_SVM: 0.533500 (0.014820)\n",
      "Scaled_SVM: 0.954500 (0.005368)\n",
      "MinMax_SVM: 0.861875 (0.009831)\n",
      "MaxAbsScaler_SVM: 0.857375 (0.009770)\n",
      "RobustScaler_SVM: 0.982375 (0.004725)\n",
      "QuantileTransformer-Normal_SVM: 0.977250 (0.002784)\n",
      "QuantileTransformer-Uniform_SVM: 0.975250 (0.004062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_SVM: 0.985000 (0.002905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_SVM: 0.796500 (0.013167)\n",
      "_SVM-PCA: 0.533500 (0.014820)\n",
      "Scaled_SVM-PCA: 0.855750 (0.009523)\n",
      "MinMax_SVM-PCA: 0.796750 (0.013219)\n",
      "MaxAbsScaler_SVM-PCA: 0.796625 (0.013370)\n",
      "RobustScaler_SVM-PCA: 0.979625 (0.004543)\n",
      "QuantileTransformer-Normal_SVM-PCA: 0.861000 (0.010837)\n",
      "QuantileTransformer-Uniform_SVM-PCA: 0.846000 (0.010305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_SVM-PCA: 0.828750 (0.010383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_SVM-PCA: 0.796500 (0.013167)\n",
      "_RF: 0.933000 (0.005948)\n",
      "Scaled_RF: 0.933000 (0.005948)\n",
      "MinMax_RF: 0.933000 (0.005948)\n",
      "MaxAbsScaler_RF: 0.933000 (0.005948)\n",
      "RobustScaler_RF: 0.933000 (0.005948)\n",
      "QuantileTransformer-Normal_RF: 0.933000 (0.005948)\n",
      "QuantileTransformer-Uniform_RF: 0.933000 (0.005948)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_RF: 0.867250 (0.013226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_RF: 0.797875 (0.012451)\n",
      "_RF-PCA: 0.933000 (0.005948)\n",
      "Scaled_RF-PCA: 0.851125 (0.012125)\n",
      "MinMax_RF-PCA: 0.788625 (0.013281)\n",
      "MaxAbsScaler_RF-PCA: 0.748125 (0.012289)\n",
      "RobustScaler_RF-PCA: 0.854125 (0.015593)\n",
      "QuantileTransformer-Normal_RF-PCA: 0.812625 (0.010193)\n",
      "QuantileTransformer-Uniform_RF-PCA: 0.830250 (0.011150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_RF-PCA: 0.676000 (0.020546)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_RF-PCA: 0.797875 (0.012451)\n",
      "_MLP: 0.616250 (0.180594)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_MLP: 0.984375 (0.005101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_MLP: 0.978000 (0.006452)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_MLP: 0.973750 (0.005916)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_MLP: 0.988875 (0.003135)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_MLP: 0.985250 (0.002610)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_MLP: 0.984500 (0.002861)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_MLP: 0.988500 (0.004176)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_MLP: 0.797750 (0.012435)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_MLP-PCA: 0.616250 (0.180594)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_MLP-PCA: 0.861375 (0.012060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_MLP-PCA: 0.798875 (0.012655)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_MLP-PCA: 0.797250 (0.012309)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_MLP-PCA: 0.985000 (0.003832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_MLP-PCA: 0.858125 (0.012161)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_MLP-PCA: 0.863875 (0.010315)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2778: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_MLP-PCA: 0.873500 (0.010648)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_MLP-PCA: 0.797875 (0.012451)\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_LR: 0.796500 (0.013167) [test_score:0.798]\n",
      "\u001b[0mScaled_LR: 0.955250 (0.006119) [test_score:0.957]\n",
      "\u001b[0mMinMax_LR: 0.895125 (0.009592) [test_score:0.894]\n",
      "\u001b[0mMaxAbsScaler_LR: 0.874750 (0.008958) [test_score:0.875]\n",
      "\u001b[0mRobustScaler_LR: 0.974750 (0.005911) [test_score:0.979]\n",
      "\u001b[0mQuantileTransformer-Normal_LR: 0.971000 (0.005148) [test_score:0.972]\n",
      "\u001b[0mQuantileTransformer-Uniform_LR: 0.954625 (0.004332) [test_score:0.957]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_LR: 0.979375 (0.004945) [test_score:0.979]\n",
      "\u001b[0mNormalizer_LR: 0.797875 (0.012451) [test_score:0.798]\n",
      "\n",
      "\u001b[0m_LR-PCA: 0.796500 (0.013167) [test_score:0.798]\n",
      "\u001b[0mScaled_LR-PCA: 0.847500 (0.010969) [test_score:0.842]\n",
      "\u001b[0mMinMax_LR-PCA: 0.796875 (0.013029) [test_score:0.797]\n",
      "\u001b[0mMaxAbsScaler_LR-PCA: 0.797125 (0.012637) [test_score:0.797]\n",
      "\u001b[1mRobustScaler_LR-PCA: 0.957250 (0.007842) [test_score:0.965]\n",
      "\u001b[0mQuantileTransformer-Normal_LR-PCA: 0.821875 (0.013326) [test_score:0.816]\n",
      "\u001b[0mQuantileTransformer-Uniform_LR-PCA: 0.821000 (0.012597) [test_score:0.821]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LR-PCA: 0.625375 (0.018346) [test_score:0.920]\n",
      "\u001b[0mNormalizer_LR-PCA: 0.797875 (0.012451) [test_score:0.798]\n",
      "\n",
      "\u001b[0m_LDA: 0.918750 (0.009953) [test_score:0.912]\n",
      "\u001b[0mScaled_LDA: 0.911500 (0.008174) [test_score:0.906]\n",
      "\u001b[0mMinMax_LDA: 0.911500 (0.008174) [test_score:0.906]\n",
      "\u001b[0mMaxAbsScaler_LDA: 0.911500 (0.008174) [test_score:0.906]\n",
      "\u001b[0mRobustScaler_LDA: 0.911500 (0.008174) [test_score:0.906]\n",
      "\u001b[1mQuantileTransformer-Normal_LDA: 0.969750 (0.004323) [test_score:0.968]\n",
      "\u001b[0mQuantileTransformer-Uniform_LDA: 0.967875 (0.006022) [test_score:0.969]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LDA: 0.962750 (0.005777) [test_score:0.975]\n",
      "\u001b[0mNormalizer_LDA: 0.905250 (0.008511) [test_score:0.912]\n",
      "\n",
      "\u001b[1m_LDA-PCA: 0.918750 (0.009953) [test_score:0.912]\n",
      "\u001b[0mScaled_LDA-PCA: 0.837625 (0.012911) [test_score:0.834]\n",
      "\u001b[0mMinMax_LDA-PCA: 0.790875 (0.013954) [test_score:0.792]\n",
      "\u001b[0mMaxAbsScaler_LDA-PCA: 0.777500 (0.012222) [test_score:0.771]\n",
      "\u001b[0mRobustScaler_LDA-PCA: 0.852750 (0.010706) [test_score:0.849]\n",
      "\u001b[0mQuantileTransformer-Normal_LDA-PCA: 0.824625 (0.015603) [test_score:0.816]\n",
      "\u001b[0mQuantileTransformer-Uniform_LDA-PCA: 0.812875 (0.012699) [test_score:0.808]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LDA-PCA: 0.623625 (0.018911) [test_score:0.907]\n",
      "\u001b[0mNormalizer_LDA-PCA: 0.797875 (0.012451) [test_score:0.798]\n",
      "\n",
      "\u001b[0m_KNN: 0.782750 (0.012696) [test_score:0.793]\n",
      "\u001b[0mScaled_KNN: 0.904250 (0.006736) [test_score:0.902]\n",
      "\u001b[0mMinMax_KNN: 0.877375 (0.009754) [test_score:0.886]\n",
      "\u001b[0mMaxAbsScaler_KNN: 0.868625 (0.009754) [test_score:0.876]\n",
      "\u001b[0mRobustScaler_KNN: 0.952625 (0.005137) [test_score:0.953]\n",
      "\u001b[0mQuantileTransformer-Normal_KNN: 0.943875 (0.009261) [test_score:0.951]\n",
      "\u001b[0mQuantileTransformer-Uniform_KNN: 0.955250 (0.006169) [test_score:0.956]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_KNN: 0.955875 (0.004509) [test_score:0.965]\n",
      "\u001b[0mNormalizer_KNN: 0.782500 (0.014098) [test_score:0.792]\n",
      "\n",
      "\u001b[0m_KNN-PCA: 0.782750 (0.012696) [test_score:0.793]\n",
      "\u001b[0mScaled_KNN-PCA: 0.845750 (0.008700) [test_score:0.849]\n",
      "\u001b[0mMinMax_KNN-PCA: 0.804875 (0.013422) [test_score:0.812]\n",
      "\u001b[0mMaxAbsScaler_KNN-PCA: 0.778500 (0.014435) [test_score:0.790]\n",
      "\u001b[1mRobustScaler_KNN-PCA: 0.970375 (0.006276) [test_score:0.972]\n",
      "\u001b[0mQuantileTransformer-Normal_KNN-PCA: 0.860500 (0.008771) [test_score:0.859]\n",
      "\u001b[0mQuantileTransformer-Uniform_KNN-PCA: 0.866500 (0.011576) [test_score:0.870]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_KNN-PCA: 0.822500 (0.013062) [test_score:0.932]\n",
      "\u001b[0mNormalizer_KNN-PCA: 0.782125 (0.013751) [test_score:0.793]\n",
      "\n",
      "\u001b[0m_CART: 0.985125 (0.003281) [test_score:0.985]\n",
      "\u001b[0mScaled_CART: 0.984750 (0.002894) [test_score:0.985]\n",
      "\u001b[0mMinMax_CART: 0.984750 (0.002947) [test_score:0.984]\n",
      "\u001b[0mMaxAbsScaler_CART: 0.984750 (0.002947) [test_score:0.984]\n",
      "\u001b[0mRobustScaler_CART: 0.984750 (0.002894) [test_score:0.985]\n",
      "\u001b[1mQuantileTransformer-Normal_CART: 0.985125 (0.003034) [test_score:0.985]\n",
      "\u001b[0mQuantileTransformer-Uniform_CART: 0.985000 (0.003211) [test_score:0.985]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_CART: 0.984375 (0.003802) [test_score:0.986]\n",
      "\u001b[0mNormalizer_CART: 0.719125 (0.015613) [test_score:0.739]\n",
      "\n",
      "\u001b[1m_CART-PCA: 0.985125 (0.003281) [test_score:0.985]\n",
      "\u001b[0mScaled_CART-PCA: 0.803750 (0.020210) [test_score:0.810]\n",
      "\u001b[0mMinMax_CART-PCA: 0.746250 (0.016441) [test_score:0.764]\n",
      "\u001b[0mMaxAbsScaler_CART-PCA: 0.718375 (0.020326) [test_score:0.735]\n",
      "\u001b[0mRobustScaler_CART-PCA: 0.971000 (0.003945) [test_score:0.980]\n",
      "\u001b[0mQuantileTransformer-Normal_CART-PCA: 0.818250 (0.014771) [test_score:0.826]\n",
      "\u001b[0mQuantileTransformer-Uniform_CART-PCA: 0.818000 (0.015484) [test_score:0.820]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_CART-PCA: 0.803125 (0.009392) [test_score:0.909]\n",
      "\u001b[0mNormalizer_CART-PCA: 0.718375 (0.015125) [test_score:0.741]\n",
      "\n",
      "\u001b[0m_NB: 0.796250 (0.012784) [test_score:0.797]\n",
      "\u001b[0mScaled_NB: 0.942000 (0.006524) [test_score:0.947]\n",
      "\u001b[0mMinMax_NB: 0.941750 (0.006643) [test_score:0.947]\n",
      "\u001b[0mMaxAbsScaler_NB: 0.941750 (0.006643) [test_score:0.947]\n",
      "\u001b[0mRobustScaler_NB: 0.942000 (0.006524) [test_score:0.947]\n",
      "\u001b[0mQuantileTransformer-Normal_NB: 0.924625 (0.011097) [test_score:0.914]\n",
      "\u001b[0mQuantileTransformer-Uniform_NB: 0.924125 (0.012210) [test_score:0.915]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_NB: 0.985125 (0.002820) [test_score:0.986]\n",
      "\u001b[0mNormalizer_NB: 0.797875 (0.012451) [test_score:0.798]\n",
      "\n",
      "\u001b[0m_NB-PCA: 0.796250 (0.012784) [test_score:0.797]\n",
      "\u001b[0mScaled_NB-PCA: 0.840500 (0.013877) [test_score:0.837]\n",
      "\u001b[0mMinMax_NB-PCA: 0.781625 (0.014959) [test_score:0.782]\n",
      "\u001b[0mMaxAbsScaler_NB-PCA: 0.768125 (0.012957) [test_score:0.763]\n",
      "\u001b[1mRobustScaler_NB-PCA: 0.880000 (0.011713) [test_score:0.874]\n",
      "\u001b[0mQuantileTransformer-Normal_NB-PCA: 0.835375 (0.013452) [test_score:0.837]\n",
      "\u001b[0mQuantileTransformer-Uniform_NB-PCA: 0.826375 (0.013115) [test_score:0.825]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_NB-PCA: 0.628500 (0.016954) [test_score:0.912]\n",
      "\u001b[0mNormalizer_NB-PCA: 0.794250 (0.013707) [test_score:0.796]\n",
      "\n",
      "\u001b[0m_SVM: 0.533500 (0.014820) [test_score:0.532]\n",
      "\u001b[0mScaled_SVM: 0.954500 (0.005368) [test_score:0.958]\n",
      "\u001b[0mMinMax_SVM: 0.861875 (0.009831) [test_score:0.861]\n",
      "\u001b[0mMaxAbsScaler_SVM: 0.857375 (0.009770) [test_score:0.856]\n",
      "\u001b[0mRobustScaler_SVM: 0.982375 (0.004725) [test_score:0.983]\n",
      "\u001b[0mQuantileTransformer-Normal_SVM: 0.977250 (0.002784) [test_score:0.979]\n",
      "\u001b[0mQuantileTransformer-Uniform_SVM: 0.975250 (0.004062) [test_score:0.975]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_SVM: 0.985000 (0.002905) [test_score:0.986]\n",
      "\u001b[0mNormalizer_SVM: 0.796500 (0.013167) [test_score:0.798]\n",
      "\n",
      "\u001b[0m_SVM-PCA: 0.533500 (0.014820) [test_score:0.532]\n",
      "\u001b[0mScaled_SVM-PCA: 0.855750 (0.009523) [test_score:0.854]\n",
      "\u001b[0mMinMax_SVM-PCA: 0.796750 (0.013219) [test_score:0.798]\n",
      "\u001b[0mMaxAbsScaler_SVM-PCA: 0.796625 (0.013370) [test_score:0.798]\n",
      "\u001b[1mRobustScaler_SVM-PCA: 0.979625 (0.004543) [test_score:0.983]\n",
      "\u001b[0mQuantileTransformer-Normal_SVM-PCA: 0.861000 (0.010837) [test_score:0.861]\n",
      "\u001b[0mQuantileTransformer-Uniform_SVM-PCA: 0.846000 (0.010305) [test_score:0.843]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_SVM-PCA: 0.828750 (0.010383) [test_score:0.939]\n",
      "\u001b[0mNormalizer_SVM-PCA: 0.796500 (0.013167) [test_score:0.798]\n",
      "\n",
      "\u001b[1m_RF: 0.933000 (0.005948) [test_score:0.928]\n",
      "\u001b[0mScaled_RF: 0.933000 (0.005948) [test_score:0.928]\n",
      "\u001b[0mMinMax_RF: 0.933000 (0.005948) [test_score:0.928]\n",
      "\u001b[0mMaxAbsScaler_RF: 0.933000 (0.005948) [test_score:0.928]\n",
      "\u001b[0mRobustScaler_RF: 0.933000 (0.005948) [test_score:0.928]\n",
      "\u001b[0mQuantileTransformer-Normal_RF: 0.933000 (0.005948) [test_score:0.928]\n",
      "\u001b[0mQuantileTransformer-Uniform_RF: 0.933000 (0.005948) [test_score:0.928]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_RF: 0.867250 (0.013226) [test_score:0.968]\n",
      "\u001b[0mNormalizer_RF: 0.797875 (0.012451) [test_score:0.798]\n",
      "\n",
      "\u001b[1m_RF-PCA: 0.933000 (0.005948) [test_score:0.928]\n",
      "\u001b[0mScaled_RF-PCA: 0.851125 (0.012125) [test_score:0.850]\n",
      "\u001b[0mMinMax_RF-PCA: 0.788625 (0.013281) [test_score:0.792]\n",
      "\u001b[0mMaxAbsScaler_RF-PCA: 0.748125 (0.012289) [test_score:0.757]\n",
      "\u001b[0mRobustScaler_RF-PCA: 0.854125 (0.015593) [test_score:0.847]\n",
      "\u001b[0mQuantileTransformer-Normal_RF-PCA: 0.812625 (0.010193) [test_score:0.802]\n",
      "\u001b[0mQuantileTransformer-Uniform_RF-PCA: 0.830250 (0.011150) [test_score:0.828]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_RF-PCA: 0.676000 (0.020546) [test_score:0.916]\n",
      "\u001b[0mNormalizer_RF-PCA: 0.797875 (0.012451) [test_score:0.799]\n",
      "\n",
      "\u001b[0m_MLP: 0.616250 (0.180594) [test_score:0.796]\n",
      "\u001b[0mScaled_MLP: 0.984375 (0.005101) [test_score:0.985]\n",
      "\u001b[0mMinMax_MLP: 0.978000 (0.006452) [test_score:0.980]\n",
      "\u001b[0mMaxAbsScaler_MLP: 0.973750 (0.005916) [test_score:0.976]\n",
      "\u001b[1mRobustScaler_MLP: 0.988875 (0.003135) [test_score:0.987]\n",
      "\u001b[0mQuantileTransformer-Normal_MLP: 0.985250 (0.002610) [test_score:0.984]\n",
      "\u001b[0mQuantileTransformer-Uniform_MLP: 0.984500 (0.002861) [test_score:0.983]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_MLP: 0.988500 (0.004176) [test_score:0.985]\n",
      "\u001b[0mNormalizer_MLP: 0.797750 (0.012435) [test_score:0.798]\n",
      "\n",
      "\u001b[0m_MLP-PCA: 0.616250 (0.180594) [test_score:0.796]\n",
      "\u001b[0mScaled_MLP-PCA: 0.861375 (0.012060) [test_score:0.863]\n",
      "\u001b[0mMinMax_MLP-PCA: 0.798875 (0.012655) [test_score:0.796]\n",
      "\u001b[0mMaxAbsScaler_MLP-PCA: 0.797250 (0.012309) [test_score:0.798]\n",
      "\u001b[1mRobustScaler_MLP-PCA: 0.985000 (0.003832) [test_score:0.986]\n",
      "\u001b[0mQuantileTransformer-Normal_MLP-PCA: 0.858125 (0.012161) [test_score:0.858]\n",
      "\u001b[0mQuantileTransformer-Uniform_MLP-PCA: 0.863875 (0.010315) [test_score:0.867]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_MLP-PCA: 0.873500 (0.010648) [test_score:0.944]\n",
      "\u001b[0mNormalizer_MLP-PCA: 0.797875 (0.012451) [test_score:0.798]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_LR',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR',Pipeline([('Scaler',MinMaxScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR',Pipeline([('Scaler',MaxAbsScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR',Pipeline([('Scaler',RobustScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LR',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LR',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LR',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR',Pipeline([('Scaler',Normalizer()),('LR',LogisticRegression())])))\n",
    "\n",
    "pipelines.append(('_LR-PCA',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LR-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LR-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LR-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_LDA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA',Pipeline([('Scaler',MinMaxScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA',Pipeline([('Scaler',MaxAbsScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA',Pipeline([('Scaler',RobustScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LDA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LDA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LDA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA',Pipeline([('Scaler',Normalizer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "pipelines.append(('_LDA-PCA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LDA-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LDA-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LDA-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_KNN',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN',Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN',Pipeline([('Scaler',MaxAbsScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN',Pipeline([('Scaler',RobustScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_KNN',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_KNN',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_KNN',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN',Pipeline([('Scaler',Normalizer()),('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "pipelines.append(('_KNN-PCA',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_KNN-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_KNN-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_KNN-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_CART',Pipeline([('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_CART',Pipeline([('Scaler',StandardScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_CART',Pipeline([('Scaler',MinMaxScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_CART',Pipeline([('Scaler',MaxAbsScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_CART',Pipeline([('Scaler',RobustScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_CART',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_CART',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_CART',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_CART',Pipeline([('Scaler',Normalizer()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_CART-PCA',Pipeline([('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_CART-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_CART-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_CART-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_CART-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_CART-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_CART-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_CART-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_CART-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_NB',Pipeline([('NB',GaussianNB())])))\n",
    "pipelines.append(('Scaled_NB',Pipeline([('Scaler',StandardScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('MinMax_NB',Pipeline([('Scaler',MinMaxScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('MaxAbsScaler_NB',Pipeline([('Scaler',MaxAbsScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('RobustScaler_NB',Pipeline([('Scaler',RobustScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_NB',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_NB',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('NB',GaussianNB())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_NB',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('NB',GaussianNB())])))\n",
    "pipelines.append(('Normalizer_NB',Pipeline([('Scaler',Normalizer()),('NB',GaussianNB())])))\n",
    "\n",
    "pipelines.append(('_NB-PCA',Pipeline([('NB',GaussianNB())])))\n",
    "pipelines.append(('Scaled_NB-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('MinMax_NB-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('MaxAbsScaler_NB-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('RobustScaler_NB-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_NB-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_NB-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_NB-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('Normalizer_NB-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_SVM' ,Pipeline([('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM',Pipeline([('Scaler',MinMaxScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM',Pipeline([('Scaler',MaxAbsScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM',Pipeline([('Scaler',RobustScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_SVM',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_SVM',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_SVM',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM',Pipeline([('Scaler',Normalizer()),('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_SVM-PCA',Pipeline([('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_SVM-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_SVM-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_SVM-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_RF' ,Pipeline([('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF',Pipeline([('Scaler',MinMaxScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF',Pipeline([('Scaler',MaxAbsScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF',Pipeline([('Scaler',RobustScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_RF',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_RF',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_RF',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF',Pipeline([('Scaler',Normalizer()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_RF-PCA',Pipeline([('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_RF-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_RF-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_RF-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_MLP',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP',Pipeline([('Scaler',StandardScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP',Pipeline([('Scaler',MinMaxScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP',Pipeline([('Scaler',MaxAbsScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP',Pipeline([('Scaler',RobustScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_MLP',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_MLP',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_MLP',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP',Pipeline([('Scaler',Normalizer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_MLP-PCA',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_MLP-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_MLP-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_MLP-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "test_scores = []\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "    # fit on train and predict on test\n",
    "    model.fit(X_train,y_train)\n",
    "    test_scores.append(model.score(X_test,y_test))\n",
    "    \n",
    "print_results(names, results, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune parameters and then check Normalization and Standartization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1539 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 5193 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   28.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1516 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4764 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   23.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 2876 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   26.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1836 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 4678 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   28.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1316 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3346 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6176 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   42.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:   48.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  3.7min finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_RF: 0.765060 (0.103615)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 4012 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   30.7s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_RF: 0.789157 (0.108115)\n",
      "Test score 0.7380952380952381\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 4012 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   34.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_RF-PCA: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 612 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1830 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3528 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 5718 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   34.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_RF-PCA: 0.722892 (0.095618)\n",
      "Test score 0.7142857142857143\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 4012 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   28.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_RF-PCA: 0.759036 (0.108783)\n",
      "Test score 0.7857142857142857\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4012 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   27.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_RF-PCA: 0.777108 (0.131756)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2680 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4944 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   41.9s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_RF-PCA: 0.777108 (0.143657)\n",
      "Test score 0.7857142857142857\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   42.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3922 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5136 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  3.6min finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_RF-PCA: 0.728916 (0.106791)\n",
      "Test score 0.7857142857142857\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1836 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 4678 tasks      | elapsed:   19.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_RF-PCA: 0.674699 (0.066384)\n",
      "Test score 0.7619047619047619\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_RF: 0.759036 (0.094511)\n",
      "\u001b[0mScaled_RF: 0.759036 (0.094511)\n",
      "\u001b[0mMinMax_RF: 0.759036 (0.094511)\n",
      "\u001b[0mMaxAbsScaler_RF: 0.759036 (0.094511)\n",
      "\u001b[0mRobustScaler_RF: 0.759036 (0.094511)\n",
      "\u001b[0mQuantileTransformer_RF: 0.765060 (0.103615)\n",
      "\u001b[1mNormalizer_RF: 0.789157 (0.108115)\n",
      "\n",
      "\u001b[0m_RF-PCA: 0.759036 (0.094511)\n",
      "\u001b[0mScaled_RF-PCA: 0.722892 (0.095618)\n",
      "\u001b[0mMinMax_RF-PCA: 0.759036 (0.108783)\n",
      "\u001b[0mMaxAbsScaler_RF-PCA: 0.777108 (0.131756)\n",
      "\u001b[0mRobustScaler_RF-PCA: 0.777108 (0.143657)\n",
      "\u001b[0mQuantileTransformer_RF-PCA: 0.728916 (0.106791)\n",
      "\u001b[0mNormalizer_RF-PCA: 0.674699 (0.066384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   27.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 20, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Create the random grid\n",
    "random_grid = {'RF__n_estimators': n_estimators,\n",
    "               'RF__max_features': max_features,\n",
    "               'RF__max_depth': max_depth,\n",
    "               'RF__min_samples_split': min_samples_split,\n",
    "               'RF__min_samples_leaf': min_samples_leaf,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_RF' ,Pipeline([('RF' , RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF',Pipeline([('Scaler',MinMaxScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF',Pipeline([('Scaler',MaxAbsScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF',Pipeline([('Scaler',RobustScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_RF',Pipeline([('Scaler',QuantileTransformer()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF',Pipeline([('Scaler',Normalizer()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_RF-PCA',Pipeline([('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_RF-PCA',Pipeline([('Scaler',QuantileTransformer()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "        \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_KNN: 0.855422 (0.047806)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_KNN: 0.825301 (0.094589)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_KNN: 0.843373 (0.061421)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_KNN: 0.843373 (0.061421)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 144 out of 320 | elapsed:    0.4s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_KNN: 0.837349 (0.073886)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    7.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_KNN: 0.813253 (0.099759)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    7.9s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_KNN: 0.891566 (0.050710)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    8.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_KNN: 0.891566 (0.058800)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_KNN: 0.801205 (0.091692)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_KNN-PCA: 0.855422 (0.047806)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_KNN-PCA: 0.819277 (0.077372)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_KNN-PCA: 0.783133 (0.135372)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_KNN-PCA: 0.783133 (0.129690)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 144 out of 320 | elapsed:    0.5s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    1.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_KNN-PCA: 0.765060 (0.086233)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    8.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_KNN-PCA: 0.734940 (0.121702)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    9.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_KNN-PCA: 0.765060 (0.115540)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   10.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_KNN-PCA: 0.759036 (0.108579)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "Normalizer_KNN-PCA: 0.716867 (0.068914)\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_KNN: 0.855422 (0.047806) [test_score:0.857]\n",
      "\u001b[0mScaled_KNN: 0.825301 (0.094589) [test_score:0.952]\n",
      "\u001b[0mMinMax_KNN: 0.843373 (0.061421) [test_score:0.857]\n",
      "\u001b[0mMaxAbsScaler_KNN: 0.843373 (0.061421) [test_score:0.857]\n",
      "\u001b[0mRobustScaler_KNN: 0.837349 (0.073886) [test_score:0.929]\n",
      "\u001b[0mQuantileTransformer-Normal_KNN: 0.813253 (0.099759) [test_score:0.881]\n",
      "\u001b[1mQuantileTransformer-Uniform_KNN: 0.891566 (0.050710) [test_score:0.905]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_KNN: 0.891566 (0.058800) [test_score:0.905]\n",
      "\u001b[0mNormalizer_KNN: 0.801205 (0.091692) [test_score:0.881]\n",
      "\n",
      "\u001b[0m_KNN-PCA: 0.855422 (0.047806) [test_score:0.857]\n",
      "\u001b[0mScaled_KNN-PCA: 0.819277 (0.077372) [test_score:0.738]\n",
      "\u001b[0mMinMax_KNN-PCA: 0.783133 (0.135372) [test_score:0.762]\n",
      "\u001b[0mMaxAbsScaler_KNN-PCA: 0.783133 (0.129690) [test_score:0.786]\n",
      "\u001b[0mRobustScaler_KNN-PCA: 0.765060 (0.086233) [test_score:0.690]\n",
      "\u001b[0mQuantileTransformer-Normal_KNN-PCA: 0.734940 (0.121702) [test_score:0.786]\n",
      "\u001b[0mQuantileTransformer-Uniform_KNN-PCA: 0.765060 (0.115540) [test_score:0.833]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_KNN-PCA: 0.759036 (0.108579) [test_score:0.833]\n",
      "\u001b[0mNormalizer_KNN-PCA: 0.716867 (0.068914) [test_score:0.786]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "n_neighbors  = [int(x) for x in np.linspace(start = 1, stop = 20, num = 2)]\n",
    "weights  = [\"uniform\",\"distance\"]\n",
    "algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "leaf_size =[int(x) for x in np.linspace(start = 5, stop = 50, num = 2)]\n",
    "p =[int(x) for x in np.linspace(start = 1, stop = 4, num = 1)]\n",
    "# Create the random grid\n",
    "random_grid = {'KNN__n_neighbors': n_neighbors,\n",
    "               'KNN__weights': weights,\n",
    "               'KNN__algorithm': algorithm,\n",
    "               'KNN__leaf_size': leaf_size,\n",
    "               'KNN__p': p,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_KNN',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN',Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN',Pipeline([('Scaler',MaxAbsScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN',Pipeline([('Scaler',RobustScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_KNN',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_KNN',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_KNN',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN',Pipeline([('Scaler',Normalizer()),('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "pipelines.append(('_KNN-PCA',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_KNN-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_KNN-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_KNN-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SVM: 0.783133 (0.114643)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 184 out of 400 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_SVM: 0.855422 (0.072632)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_SVM: 0.783133 (0.134634)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_SVM: 0.783133 (0.097068)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    1.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_SVM: 0.801205 (0.089616)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   10.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_SVM: 0.855422 (0.080169)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    9.9s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_SVM: 0.813253 (0.107352)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   10.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_SVM: 0.867470 (0.096210)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_SVM: 0.710843 (0.139746)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 184 out of 400 | elapsed:    0.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SVM-PCA: 0.783133 (0.114643)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    1.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_SVM-PCA: 0.783133 (0.068430)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_SVM-PCA: 0.777108 (0.109443)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_SVM-PCA: 0.771084 (0.129133)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    1.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_SVM-PCA: 0.795181 (0.108223)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   10.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_SVM-PCA: 0.734940 (0.086618)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    9.9s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_SVM-PCA: 0.789157 (0.099026)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   11.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_SVM-PCA: 0.759036 (0.115976)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n",
      "Normalizer_SVM-PCA: 0.632530 (0.088912)\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_SVM: 0.783133 (0.114643) [test_score:0.738]\n",
      "\u001b[0mScaled_SVM: 0.855422 (0.072632) [test_score:0.833]\n",
      "\u001b[0mMinMax_SVM: 0.783133 (0.134634) [test_score:0.738]\n",
      "\u001b[0mMaxAbsScaler_SVM: 0.783133 (0.097068) [test_score:0.762]\n",
      "\u001b[0mRobustScaler_SVM: 0.801205 (0.089616) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Normal_SVM: 0.855422 (0.080169) [test_score:0.857]\n",
      "\u001b[0mQuantileTransformer-Uniform_SVM: 0.813253 (0.107352) [test_score:0.762]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_SVM: 0.867470 (0.096210) [test_score:0.881]\n",
      "\u001b[0mNormalizer_SVM: 0.710843 (0.139746) [test_score:0.714]\n",
      "\n",
      "\u001b[0m_SVM-PCA: 0.783133 (0.114643) [test_score:0.738]\n",
      "\u001b[0mScaled_SVM-PCA: 0.783133 (0.068430) [test_score:0.738]\n",
      "\u001b[0mMinMax_SVM-PCA: 0.777108 (0.109443) [test_score:0.690]\n",
      "\u001b[0mMaxAbsScaler_SVM-PCA: 0.771084 (0.129133) [test_score:0.714]\n",
      "\u001b[0mRobustScaler_SVM-PCA: 0.795181 (0.108223) [test_score:0.738]\n",
      "\u001b[0mQuantileTransformer-Normal_SVM-PCA: 0.734940 (0.086618) [test_score:0.762]\n",
      "\u001b[0mQuantileTransformer-Uniform_SVM-PCA: 0.789157 (0.099026) [test_score:0.738]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_SVM-PCA: 0.759036 (0.115976) [test_score:0.738]\n",
      "\u001b[0mNormalizer_SVM-PCA: 0.632530 (0.088912) [test_score:0.571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "C  = [x for x in np.arange(0.1, 2, 0.2)]\n",
    "kernel   = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'SVM__C': C,\n",
    "               'SVM__kernel': kernel,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_SVM' ,Pipeline([('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM',Pipeline([('Scaler',MinMaxScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM',Pipeline([('Scaler',MaxAbsScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM',Pipeline([('Scaler',RobustScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_SVM',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_SVM',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_SVM',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM',Pipeline([('Scaler',Normalizer()),('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_SVM-PCA',Pipeline([('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_SVM-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_SVM-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_SVM-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "        \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LR: 0.777108 (0.085984)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LR: 0.753012 (0.103062)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LR: 0.783133 (0.111211)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.7s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LR: 0.789157 (0.111642)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LR: 0.783133 (0.110311)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   14.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_LR: 0.771084 (0.135003)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   16.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_LR: 0.813253 (0.116741)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   21.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_LR: 0.801205 (0.110630)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.7s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LR: 0.746988 (0.124564)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LR-PCA: 0.777108 (0.085984)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LR-PCA: 0.777108 (0.124496)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LR-PCA: 0.765060 (0.124579)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LR-PCA: 0.771084 (0.128618)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    2.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LR-PCA: 0.746988 (0.131147)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   25.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_LR-PCA: 0.771084 (0.143357)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   22.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_LR-PCA: 0.801205 (0.102417)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   25.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_LR-PCA: 0.783133 (0.120667)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LR-PCA: 0.656627 (0.084885)\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_LR: 0.777108 (0.085984) [test_score:0.810]\n",
      "\u001b[0mScaled_LR: 0.753012 (0.103062) [test_score:0.762]\n",
      "\u001b[0mMinMax_LR: 0.783133 (0.111211) [test_score:0.762]\n",
      "\u001b[0mMaxAbsScaler_LR: 0.789157 (0.111642) [test_score:0.762]\n",
      "\u001b[0mRobustScaler_LR: 0.783133 (0.110311) [test_score:0.738]\n",
      "\u001b[0mQuantileTransformer-Normal_LR: 0.771084 (0.135003) [test_score:0.810]\n",
      "\u001b[1mQuantileTransformer-Uniform_LR: 0.813253 (0.116741) [test_score:0.762]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LR: 0.801205 (0.110630) [test_score:0.810]\n",
      "\u001b[0mNormalizer_LR: 0.746988 (0.124564) [test_score:0.690]\n",
      "\n",
      "\u001b[0m_LR-PCA: 0.777108 (0.085984) [test_score:0.810]\n",
      "\u001b[0mScaled_LR-PCA: 0.777108 (0.124496) [test_score:0.738]\n",
      "\u001b[0mMinMax_LR-PCA: 0.765060 (0.124579) [test_score:0.690]\n",
      "\u001b[0mMaxAbsScaler_LR-PCA: 0.771084 (0.128618) [test_score:0.714]\n",
      "\u001b[0mRobustScaler_LR-PCA: 0.746988 (0.131147) [test_score:0.714]\n",
      "\u001b[0mQuantileTransformer-Normal_LR-PCA: 0.771084 (0.143357) [test_score:0.714]\n",
      "\u001b[0mQuantileTransformer-Uniform_LR-PCA: 0.801205 (0.102417) [test_score:0.738]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LR-PCA: 0.783133 (0.120667) [test_score:0.714]\n",
      "\u001b[0mNormalizer_LR-PCA: 0.656627 (0.084885) [test_score:0.571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "C  = [x for x in np.arange(0.1, 3, 0.2)]\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "# dual = [True, False]\n",
    "fit_intercept = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'LR__C': C,\n",
    "               'LR__penalty': penalty,\n",
    "#                'LR__dual': dual,\n",
    "               'LR__fit_intercept': fit_intercept\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_LR',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR',Pipeline([('Scaler',MinMaxScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR',Pipeline([('Scaler',MaxAbsScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR',Pipeline([('Scaler',RobustScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LR',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LR',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LR',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR',Pipeline([('Scaler',Normalizer()),('LR',LogisticRegression())])))\n",
    "\n",
    "pipelines.append(('_LR-PCA',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LR-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LR-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LR-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "        \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LDA: 0.789157 (0.133339)\n",
      "Test score 0.6904761904761905\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LDA: 0.777108 (0.091717)\n",
      "Test score 0.8095238095238095\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LDA: 0.777108 (0.095270)\n",
      "Test score 0.7380952380952381\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LDA: 0.777108 (0.095270)\n",
      "Test score 0.7380952380952381\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LDA: 0.777108 (0.092079)\n",
      "Test score 0.7857142857142857\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    3.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_LDA: 0.807229 (0.104187)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LDA: 0.746988 (0.121868)\n",
      "Test score 0.6904761904761905\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LDA-PCA: 0.789157 (0.133339)\n",
      "Test score 0.6904761904761905\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LDA-PCA: 0.771084 (0.136876)\n",
      "Test score 0.7142857142857143\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LDA-PCA: 0.771084 (0.128618)\n",
      "Test score 0.6666666666666666\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LDA-PCA: 0.771084 (0.128618)\n",
      "Test score 0.6904761904761905\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LDA-PCA: 0.740964 (0.122839)\n",
      "Test score 0.7142857142857143\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    3.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_LDA-PCA: 0.789157 (0.102542)\n",
      "Test score 0.7619047619047619\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n",
      "Normalizer_LDA-PCA: 0.656627 (0.086051)\n",
      "Test score 0.5952380952380952\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_LDA: 0.789157 (0.133339)\n",
      "\u001b[0mScaled_LDA: 0.777108 (0.091717)\n",
      "\u001b[0mMinMax_LDA: 0.777108 (0.095270)\n",
      "\u001b[0mMaxAbsScaler_LDA: 0.777108 (0.095270)\n",
      "\u001b[0mRobustScaler_LDA: 0.777108 (0.092079)\n",
      "\u001b[1mQuantileTransformer_LDA: 0.807229 (0.104187)\n",
      "\u001b[0mNormalizer_LDA: 0.746988 (0.121868)\n",
      "\n",
      "\u001b[0m_LDA-PCA: 0.789157 (0.133339)\n",
      "\u001b[0mScaled_LDA-PCA: 0.771084 (0.136876)\n",
      "\u001b[0mMinMax_LDA-PCA: 0.771084 (0.128618)\n",
      "\u001b[0mMaxAbsScaler_LDA-PCA: 0.771084 (0.128618)\n",
      "\u001b[0mRobustScaler_LDA-PCA: 0.740964 (0.122839)\n",
      "\u001b[0mQuantileTransformer_LDA-PCA: 0.789157 (0.102542)\n",
      "\u001b[0mNormalizer_LDA-PCA: 0.656627 (0.086051)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "solver  = [\"lsqr\", \"eigen\"]\n",
    "shrinkage = [\"auto\",None, 0.1,0.3,0.5,0.7,0.9]\n",
    "# Create the random grid\n",
    "random_grid = {'LDA__solver': solver,\n",
    "               'LDA__shrinkage': shrinkage\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_LDA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA',Pipeline([('Scaler',MinMaxScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA',Pipeline([('Scaler',MaxAbsScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA',Pipeline([('Scaler',RobustScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer_LDA',Pipeline([('Scaler',QuantileTransformer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA',Pipeline([('Scaler',Normalizer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "pipelines.append(('_LDA-PCA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer_LDA-PCA',Pipeline([('Scaler',QuantileTransformer()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    \n",
    "    print(msg)\n",
    "        \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune Multi Layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2484 candidates, totalling 24840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3922 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5136 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6514 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8052 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9754 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11616 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14162 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 17858 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 22558 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 24840 out of 24840 | elapsed:  8.0min finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_MLP: 0.849398 (0.066538)\n",
      "Fitting 10 folds for each of 2484 candidates, totalling 24840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3922 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5136 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6514 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8052 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9754 tasks      | elapsed:  3.6min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-204dd524598d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mbest_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import itertools\n",
    "\n",
    "hidden_layer_sizes = [(x,y) for x,y in itertools.product([x for x in range(1,3)],[x for x in range(5,120,5)])]\n",
    "activation = [ \"tanh\", \"relu\"]\n",
    "solver = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "alpha = [0.1,0.001,0.0001]\n",
    "learning_rate = [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "# Create the random grid\n",
    "random_grid = {'MLP__hidden_layer_sizes': hidden_layer_sizes,\n",
    "               'MLP__activation': activation,\n",
    "               'MLP__solver': solver,\n",
    "               'MLP__alpha': alpha,\n",
    "               'MLP__learning_rate': learning_rate,\n",
    "               'MLP__hidden_layer_sizes': hidden_layer_sizes,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_MLP',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP',Pipeline([('Scaler',StandardScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP',Pipeline([('Scaler',MinMaxScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP',Pipeline([('Scaler',MaxAbsScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP',Pipeline([('Scaler',RobustScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_MLP',Pipeline([('Scaler',QuantileTransformer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP',Pipeline([('Scaler',Normalizer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_MLP-PCA',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_MLP-PCA',Pipeline([('Scaler',QuantileTransformer()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to C:\\Users\\User\\scikit_learn_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5082645 , 0.60330576, 0.6198347 , ..., 0.33471075, 0.3429752 ,\n",
       "        0.3429752 ],\n",
       "       [0.78099173, 0.7768595 , 0.77272725, ..., 0.1694215 , 0.1694215 ,\n",
       "        0.1694215 ],\n",
       "       [0.59504133, 0.661157  , 0.69008267, ..., 0.17355372, 0.20661157,\n",
       "        0.17355372],\n",
       "       ...,\n",
       "       [0.45454547, 0.3677686 , 0.23966943, ..., 0.446281  , 0.45041323,\n",
       "        0.45454547],\n",
       "       [0.14876033, 0.14876033, 0.14876033, ..., 0.4876033 , 0.46694216,\n",
       "        0.27272728],\n",
       "       [0.61157024, 0.72727275, 0.74380165, ..., 0.37190083, 0.47933885,\n",
       "        0.6694215 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_covtype, fetch_olivetti_faces\n",
    "\n",
    "fetched_dataset = fetch_olivetti_faces()\n",
    "# iris = load_iris()\n",
    "X, y = fetched_dataset.data, fetched_dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train as Dataframe\n",
    "# pd.DataFrame(X_train, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.965625 (0.025958)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.975000 (0.027243)\n",
      "KNN: 0.740625 (0.102746)\n",
      "CART: 0.468750 (0.069877)\n",
      "NB: 0.759375 (0.092755)\n",
      "SVM: 0.021875 (0.044305)\n",
      "RF: 0.246875 (0.088884)\n"
     ]
    }
   ],
   "source": [
    "num_folds=10\n",
    "scoring='accuracy'\n",
    "models = []\n",
    "models.append(('LR',  LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART',DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('NB',  GaussianNB()))\n",
    "models.append(('SVM', SVC(random_state=seed)))\n",
    "models.append(('RF',  RandomForestClassifier(max_depth=3, random_state=seed)))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: 0.934375 (0.066218)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLDA: 0.975000 (0.027243)\n",
      "ScaledKNN: 0.725000 (0.093541)\n",
      "ScaledCART: 0.475000 (0.075000)\n",
      "ScaledNB: 0.759375 (0.092755)\n",
      "ScaledSVM: 0.881250 (0.065252)\n",
      "ScaledRF: 0.246875 (0.088884)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART',Pipeline([('Scaler',StandardScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('ScaledNB',Pipeline([('Scaler',StandardScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('ScaledRF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No normalization or standartization\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "12.50%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.00%\n",
      "\n",
      "##################################################\n",
      "Data after standard scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.75%\n",
      "\n",
      "##################################################\n",
      "Data after min-max scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "12.81%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.00%\n",
      "\n",
      "##################################################\n",
      "Data after max-abs scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "12.50%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.00%\n",
      "\n",
      "##################################################\n",
      "Data after robust scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.75%\n",
      "\n",
      "##################################################\n",
      "Data after power transformation (Yeo-Johnson)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.75%\n",
      "\n",
      "##################################################\n",
      "Data after quantile transformation (gaussian pdf)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.75%\n",
      "\n",
      "##################################################\n",
      "Data after quantile transformation (uniform pdf)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "55.31%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "25.00%\n",
      "\n",
      "##################################################\n",
      "Data after sample-wise L2 normalizing\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "12.50%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.00%\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "\n",
    "# std_scale = StandardScaler().fit(X_train)\n",
    "distributions = [\n",
    "    ('Data after standard scaling',\n",
    "        StandardScaler()),\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler()),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler()),\n",
    "    ('Data after robust scaling',\n",
    "        RobustScaler(quantile_range=(25, 75))),\n",
    "    ('Data after power transformation (Yeo-Johnson)',\n",
    "     PowerTransformer(method='yeo-johnson')),\n",
    "#     ('Data after power transformation (Box-Cox)',\n",
    "#      PowerTransformer(method='box-cox')),\n",
    "    ('Data after quantile transformation (gaussian pdf)',\n",
    "        QuantileTransformer(output_distribution='normal')\n",
    "        ),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')\n",
    "        ),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer()),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(\"No normalization or standartization\")\n",
    "svc_scaled = SVC(C=1.5, random_state=seed)\n",
    "fit_std = svc_scaled.fit(X_train, y_train)\n",
    "pred_train_std = svc_scaled.predict(X_train)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "pred_test_std = svc_scaled.predict(X_test)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "\n",
    "\n",
    "print(\"#\"*50)\n",
    "\n",
    "    \n",
    "for name, dist in distributions:\n",
    "    print(name)\n",
    "    std_scale = dist.fit(X_train)\n",
    "    X_train_std = std_scale.transform(X_train)\n",
    "    X_test_std = std_scale.transform(X_test)\n",
    "\n",
    "    # on standardized data\n",
    "    svc_scaled = SVC(C=1.5, random_state=seed)\n",
    "    fit_std = svc_scaled.fit(X_train_std, y_train)\n",
    "    pred_train_std = svc_scaled.predict(X_train_std)\n",
    "\n",
    "    print('\\nPrediction accuracy for the training dataset')\n",
    "    print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "    \n",
    "    pred_test_std = svc_scaled.predict(X_test_std)\n",
    "\n",
    "    print('\\nPrediction accuracy for the test dataset')\n",
    "    print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "    \n",
    "    \n",
    "    print(\"#\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f35f8845e5ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1303\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_LR',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR',Pipeline([('Scaler',MinMaxScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR',Pipeline([('Scaler',MaxAbsScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR',Pipeline([('Scaler',RobustScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer_LR',Pipeline([('Scaler',QuantileTransformer()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR',Pipeline([('Scaler',Normalizer()),('LR',LogisticRegression())])))\n",
    "\n",
    "pipelines.append(('_LDA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA',Pipeline([('Scaler',MinMaxScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA',Pipeline([('Scaler',MaxAbsScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA',Pipeline([('Scaler',RobustScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer_LDA',Pipeline([('Scaler',QuantileTransformer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA',Pipeline([('Scaler',Normalizer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "pipelines.append(('_KNN',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN',Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN',Pipeline([('Scaler',MaxAbsScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN',Pipeline([('Scaler',RobustScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer_KNN',Pipeline([('Scaler',QuantileTransformer()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN',Pipeline([('Scaler',Normalizer()),('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "pipelines.append(('_CART',Pipeline([('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_CART',Pipeline([('Scaler',StandardScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_CART',Pipeline([('Scaler',MinMaxScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_CART',Pipeline([('Scaler',MaxAbsScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_CART',Pipeline([('Scaler',RobustScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_CART',Pipeline([('Scaler',QuantileTransformer()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_CART',Pipeline([('Scaler',Normalizer()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_NB',Pipeline([('NB',GaussianNB())])))\n",
    "pipelines.append(('Scaled_NB',Pipeline([('Scaler',StandardScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('MinMax_NB',Pipeline([('Scaler',MinMaxScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('MaxAbsScaler_NB',Pipeline([('Scaler',MaxAbsScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('RobustScaler_NB',Pipeline([('Scaler',RobustScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer_NB',Pipeline([('Scaler',QuantileTransformer()),('NB',GaussianNB())])))\n",
    "pipelines.append(('Normalizer_NB',Pipeline([('Scaler',Normalizer()),('NB',GaussianNB())])))\n",
    "\n",
    "pipelines.append(('_NB_PCA',Pipeline([('NB',GaussianNB())])))\n",
    "pipelines.append(('Scaled_NB_PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('MinMax_NB_PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('MaxAbsScaler_NB_PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('RobustScaler_NB_PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer_NB_PCA',Pipeline([('Scaler',QuantileTransformer()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('Normalizer_NB_PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "\n",
    "\n",
    "pipelines.append(('_SVM' ,Pipeline([('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM',Pipeline([('Scaler',MinMaxScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM',Pipeline([('Scaler',MaxAbsScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM',Pipeline([('Scaler',RobustScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_SVM',Pipeline([('Scaler',QuantileTransformer()), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM',Pipeline([('Scaler',Normalizer()), ('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_RF' ,Pipeline([('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF',Pipeline([('Scaler',MinMaxScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF',Pipeline([('Scaler',MaxAbsScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF',Pipeline([('Scaler',RobustScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_RF',Pipeline([('Scaler',QuantileTransformer()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF',Pipeline([('Scaler',Normalizer()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_MLP',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP',Pipeline([('Scaler',StandardScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP',Pipeline([('Scaler',MinMaxScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP',Pipeline([('Scaler',MaxAbsScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP',Pipeline([('Scaler',RobustScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_MLP',Pipeline([('Scaler',QuantileTransformer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP',Pipeline([('Scaler',Normalizer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "print_results(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_LR: 0.965625 (0.025958)\n",
      "\u001b[0mScaled_LR: 0.934375 (0.066218)\n",
      "\u001b[0mMinMax_LR: 0.968750 (0.019764)\n",
      "\u001b[0mMaxAbsScaler_LR: 0.965625 (0.025958)\n",
      "\u001b[1mRobustScaler_LR: 0.978125 (0.020010)\n",
      "\u001b[0mQuantileTransformer_LR: 0.971875 (0.021875)\n",
      "\u001b[0mNormalizer_LR: 0.071875 (0.031406)\n",
      "\n",
      "\u001b[1m_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mScaled_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mMinMax_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mMaxAbsScaler_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mRobustScaler_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mQuantileTransformer_LDA: 0.975000 (0.018750)\n",
      "\u001b[0mNormalizer_LDA: 0.956250 (0.031869)\n",
      "\n",
      "\u001b[1m_KNN: 0.740625 (0.102746)\n",
      "\u001b[0mScaled_KNN: 0.725000 (0.093541)\n",
      "\u001b[0mMinMax_KNN: 0.728125 (0.105558)\n",
      "\u001b[0mMaxAbsScaler_KNN: 0.725000 (0.091430)\n",
      "\u001b[0mRobustScaler_KNN: 0.734375 (0.079365)\n",
      "\u001b[0mQuantileTransformer_KNN: 0.725000 (0.100584)\n",
      "\u001b[0mNormalizer_KNN: 0.706250 (0.103833)\n",
      "\n",
      "\u001b[0m_CART: 0.468750 (0.069877)\n",
      "\u001b[0mScaled_CART: 0.475000 (0.075000)\n",
      "\u001b[0mMinMax_CART: 0.471875 (0.071875)\n",
      "\u001b[0mMaxAbsScaler_CART: 0.465625 (0.067676)\n",
      "\u001b[0mRobustScaler_CART: 0.471875 (0.071875)\n",
      "\u001b[1mQuantileTransformer_CART: 0.478125 (0.079119)\n",
      "\u001b[0mNormalizer_CART: 0.443750 (0.090355)\n",
      "\n",
      "\u001b[1m_NB: 0.759375 (0.092755)\n",
      "\u001b[0mScaled_NB: 0.759375 (0.092755)\n",
      "\u001b[0mMinMax_NB: 0.759375 (0.092755)\n",
      "\u001b[0mMaxAbsScaler_NB: 0.759375 (0.092755)\n",
      "\u001b[0mRobustScaler_NB: 0.759375 (0.092755)\n",
      "\u001b[0mQuantileTransformer_NB: 0.750000 (0.090571)\n",
      "\u001b[0mNormalizer_NB: 0.737500 (0.070156)\n",
      "\n",
      "\u001b[0m_SVM: 0.021875 (0.044305)\n",
      "\u001b[1mScaled_SVM: 0.881250 (0.065252)\n",
      "\u001b[0mMinMax_SVM: 0.031250 (0.041926)\n",
      "\u001b[0mMaxAbsScaler_SVM: 0.021875 (0.044305)\n",
      "\u001b[0mRobustScaler_SVM: 0.806250 (0.116760)\n",
      "\u001b[0mQuantileTransformer_SVM: 0.071875 (0.046456)\n",
      "\u001b[0mNormalizer_SVM: 0.018750 (0.040020)\n",
      "\n",
      "\u001b[1m_RF: 0.246875 (0.088884)\n",
      "\u001b[0mScaled_RF: 0.246875 (0.088884)\n",
      "\u001b[0mMinMax_RF: 0.243750 (0.093541)\n",
      "\u001b[0mMaxAbsScaler_RF: 0.243750 (0.093541)\n",
      "\u001b[0mRobustScaler_RF: 0.243750 (0.093541)\n",
      "\u001b[0mQuantileTransformer_RF: 0.240625 (0.100827)\n",
      "\u001b[0mNormalizer_RF: 0.237500 (0.065848)\n",
      "\n",
      "\u001b[0m_MLP: 0.637500 (0.248118)\n",
      "\u001b[0mScaled_MLP: 0.931250 (0.043750)\n",
      "\u001b[0mMinMax_MLP: 0.865625 (0.075325)\n",
      "\u001b[0mMaxAbsScaler_MLP: 0.462500 (0.203389)\n",
      "\u001b[0mRobustScaler_MLP: 0.925000 (0.048814)\n",
      "\u001b[1mQuantileTransformer_MLP: 0.937500 (0.050389)\n",
      "\u001b[0mNormalizer_MLP: 0.562500 (0.142522)\n"
     ]
    }
   ],
   "source": [
    "print_results(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   56.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_RF: 0.850000 (0.058962)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 660 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 943 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1308 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1753 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2280 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2887 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3576 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4345 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5196 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6127 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_RF: 0.853125 (0.059375)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2699 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3226 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3833 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4522 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5291 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6142 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_RF: 0.843750 (0.057622)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2849 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3376 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3983 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4672 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5441 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6292 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_RF: 0.850000 (0.058962)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed: 19.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_RF: 0.850000 (0.058962)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 31.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 44.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 60.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 79.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 100.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 124.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 150.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 178.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed: 198.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_RF: 0.825000 (0.050775)\n",
      "Test score 0.9\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 645 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 928 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1293 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1738 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2265 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2872 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3561 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4330 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5181 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6112 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_RF: 0.825000 (0.081729)\n",
      "Test score 0.875\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_RF: 0.850000 (0.058962)\n",
      "\u001b[1mScaled_RF: 0.853125 (0.059375)\n",
      "\u001b[0mMinMax_RF: 0.843750 (0.057622)\n",
      "\u001b[0mMaxAbsScaler_RF: 0.850000 (0.058962)\n",
      "\u001b[0mRobustScaler_RF: 0.850000 (0.058962)\n",
      "\u001b[0mQuantileTransformer_RF: 0.825000 (0.050775)\n",
      "\u001b[0mNormalizer_RF: 0.825000 (0.081729)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 20, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Create the random grid\n",
    "random_grid = {'RF__n_estimators': n_estimators,\n",
    "               'RF__max_features': max_features,\n",
    "               'RF__max_depth': max_depth,\n",
    "               'RF__min_samples_split': min_samples_split,\n",
    "               'RF__min_samples_leaf': min_samples_leaf,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_RF' ,Pipeline([('RF' , RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF',Pipeline([('Scaler',MinMaxScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF',Pipeline([('Scaler',MaxAbsScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF',Pipeline([('Scaler',RobustScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_RF',Pipeline([('Scaler',QuantileTransformer()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF',Pipeline([('Scaler',Normalizer()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    print(\"Test score\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print_results2(names, results_mean,results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SVM: 0.962500 (0.033657)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_SVM: 0.953125 (0.037630)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_SVM: 0.956250 (0.034799)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_SVM: 0.962500 (0.033657)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_SVM: 0.959375 (0.034375)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 13.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_SVM: 0.959375 (0.039652)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   37.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_SVM: 0.028125 (0.038145)\n",
      "Test score 0.0\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[1m_SVM: 0.962500 (0.033657)\n",
      "\u001b[0mScaled_SVM: 0.953125 (0.037630)\n",
      "\u001b[0mMinMax_SVM: 0.956250 (0.034799)\n",
      "\u001b[1mMaxAbsScaler_SVM: 0.962500 (0.033657)\n",
      "\u001b[0mRobustScaler_SVM: 0.959375 (0.034375)\n",
      "\u001b[0mQuantileTransformer_SVM: 0.959375 (0.039652)\n",
      "\u001b[0mNormalizer_SVM: 0.028125 (0.038145)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "C  = [x for x in np.arange(0.1, 2, 0.2)]\n",
    "kernel   = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'SVM__C': C,\n",
    "               'SVM__kernel': kernel,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_SVM' ,Pipeline([('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM',Pipeline([('Scaler',MinMaxScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM',Pipeline([('Scaler',MaxAbsScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM',Pipeline([('Scaler',RobustScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_SVM',Pipeline([('Scaler',QuantileTransformer()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM',Pipeline([('Scaler',Normalizer()),('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    print(\"Test score\", clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   41.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_KNN: 0.921875 (0.048914)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   45.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_KNN: 0.925000 (0.046771)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   43.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_KNN: 0.918750 (0.042390)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   44.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_KNN: 0.918750 (0.046771)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_KNN: 0.925000 (0.046771)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_KNN: 0.925000 (0.048814)\n",
      "Test score 0.9625\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   41.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_KNN: 0.912500 (0.053765)\n",
      "Test score 0.9875\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_KNN: 0.921875 (0.048914)\n",
      "\u001b[1mScaled_KNN: 0.925000 (0.046771)\n",
      "\u001b[0mMinMax_KNN: 0.918750 (0.042390)\n",
      "\u001b[0mMaxAbsScaler_KNN: 0.918750 (0.046771)\n",
      "\u001b[1mRobustScaler_KNN: 0.925000 (0.046771)\n",
      "\u001b[1mQuantileTransformer_KNN: 0.925000 (0.048814)\n",
      "\u001b[0mNormalizer_KNN: 0.912500 (0.053765)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "n_neighbors  = [int(x) for x in np.linspace(start = 1, stop = 20, num = 2)]\n",
    "weights  = [\"uniform\",\"distance\"]\n",
    "algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "leaf_size =[int(x) for x in np.linspace(start = 5, stop = 50, num = 2)]\n",
    "p =[int(x) for x in np.linspace(start = 1, stop = 4, num = 1)]\n",
    "# Create the random grid\n",
    "random_grid = {'KNN__n_neighbors': n_neighbors,\n",
    "               'KNN__weights': weights,\n",
    "               'KNN__algorithm': algorithm,\n",
    "               'KNN__leaf_size': leaf_size,\n",
    "               'KNN__p': p,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_KNN',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN',Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN',Pipeline([('Scaler',MaxAbsScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN',Pipeline([('Scaler',RobustScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer_KNN',Pipeline([('Scaler',QuantileTransformer()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN',Pipeline([('Scaler',Normalizer()),('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    print(\"Test score\", clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2484 candidates, totalling 24840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 27.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 46.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 55.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 63.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 74.7min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-cac57eeec1b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mbest_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import itertools\n",
    "\n",
    "hidden_layer_sizes = [(x,y) for x,y in itertools.product([x for x in range(1,3)],[x for x in range(5,120,5)])]\n",
    "activation = [ \"tanh\", \"relu\"]\n",
    "solver = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "alpha = [0.1,0.001,0.0001]\n",
    "learning_rate = [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "# Create the random grid\n",
    "random_grid = {'MLP__hidden_layer_sizes': hidden_layer_sizes,\n",
    "               'MLP__activation': activation,\n",
    "               'MLP__solver': solver,\n",
    "               'MLP__alpha': alpha,\n",
    "               'MLP__learning_rate': learning_rate,\n",
    "               'MLP__hidden_layer_sizes': hidden_layer_sizes,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_MLP',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP',Pipeline([('Scaler',StandardScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP',Pipeline([('Scaler',MinMaxScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP',Pipeline([('Scaler',MaxAbsScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP',Pipeline([('Scaler',RobustScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_MLP',Pipeline([('Scaler',QuantileTransformer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP',Pipeline([('Scaler',Normalizer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "\n",
    "print_results2(names, results_mean,results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
