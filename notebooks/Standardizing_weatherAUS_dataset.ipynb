{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load the Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_name = \"weatherAUS.csv\"\n",
    "data_path = os.path.join(\"..\",\"data\",\"raw\",file_name)\n",
    "data = pd.read_csv(data_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142193, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RISK_MM</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-12-06</td>\n",
       "      <td>Albury</td>\n",
       "      <td>14.6</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>56.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>1005.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.6</td>\n",
       "      <td>28.9</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-12-07</td>\n",
       "      <td>Albury</td>\n",
       "      <td>14.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>50.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1009.6</td>\n",
       "      <td>1008.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>24.6</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008-12-08</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.7</td>\n",
       "      <td>26.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>35.0</td>\n",
       "      <td>SSE</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1013.4</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.3</td>\n",
       "      <td>25.5</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008-12-09</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.7</td>\n",
       "      <td>31.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNW</td>\n",
       "      <td>80.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1008.9</td>\n",
       "      <td>1003.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.3</td>\n",
       "      <td>30.2</td>\n",
       "      <td>No</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008-12-10</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>28.0</td>\n",
       "      <td>S</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>1005.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.1</td>\n",
       "      <td>28.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "5  2008-12-06   Albury     14.6     29.7       0.2          NaN       NaN   \n",
       "6  2008-12-07   Albury     14.3     25.0       0.0          NaN       NaN   \n",
       "7  2008-12-08   Albury      7.7     26.7       0.0          NaN       NaN   \n",
       "8  2008-12-09   Albury      9.7     31.9       0.0          NaN       NaN   \n",
       "9  2008-12-10   Albury     13.1     30.1       1.4          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am      ...      Humidity3pm  \\\n",
       "0           W           44.0          W      ...             22.0   \n",
       "1         WNW           44.0        NNW      ...             25.0   \n",
       "2         WSW           46.0          W      ...             30.0   \n",
       "3          NE           24.0         SE      ...             16.0   \n",
       "4           W           41.0        ENE      ...             33.0   \n",
       "5         WNW           56.0          W      ...             23.0   \n",
       "6           W           50.0         SW      ...             19.0   \n",
       "7           W           35.0        SSE      ...             19.0   \n",
       "8         NNW           80.0         SE      ...              9.0   \n",
       "9           W           28.0          S      ...             27.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
       "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
       "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
       "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
       "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
       "5       1009.2       1005.4       NaN       NaN     20.6     28.9         No   \n",
       "6       1009.6       1008.2       1.0       NaN     18.1     24.6         No   \n",
       "7       1013.4       1010.1       NaN       NaN     16.3     25.5         No   \n",
       "8       1008.9       1003.6       NaN       NaN     18.3     30.2         No   \n",
       "9       1007.0       1005.7       NaN       NaN     20.1     28.2        Yes   \n",
       "\n",
       "   RISK_MM  RainTomorrow  \n",
       "0      0.0            No  \n",
       "1      0.0            No  \n",
       "2      0.0            No  \n",
       "3      1.0            No  \n",
       "4      0.2            No  \n",
       "5      0.0            No  \n",
       "6      0.0            No  \n",
       "7      0.0            No  \n",
       "8      1.4           Yes  \n",
       "9      0.0            No  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              object\n",
       "Location          object\n",
       "MinTemp          float64\n",
       "MaxTemp          float64\n",
       "Rainfall         float64\n",
       "Evaporation      float64\n",
       "Sunshine         float64\n",
       "WindGustDir       object\n",
       "WindGustSpeed    float64\n",
       "WindDir9am        object\n",
       "WindDir3pm        object\n",
       "WindSpeed9am     float64\n",
       "WindSpeed3pm     float64\n",
       "Humidity9am      float64\n",
       "Humidity3pm      float64\n",
       "Pressure9am      float64\n",
       "Pressure3pm      float64\n",
       "Cloud9am         float64\n",
       "Cloud3pm         float64\n",
       "Temp9am          float64\n",
       "Temp3pm          float64\n",
       "RainToday         object\n",
       "RISK_MM          float64\n",
       "RainTomorrow      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56420, 23)\n"
     ]
    }
   ],
   "source": [
    "label_col = \"RainTomorrow\"\n",
    "data_cleaned = data.dropna()\n",
    "X, y = data_cleaned.drop(columns=[label_col]), data_cleaned[label_col]\n",
    "print(X.dropna().shape)\n",
    "X_num = X.loc[:, X.dtypes == np.float64].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "label_col = data.shape[1]-1\n",
    "X_sonar = X_num.values.astype(float)\n",
    "y_sonar = y.values\n",
    "y_sonar = column_or_1d(y_sonar, warn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.9, 35.2,  0. , ..., 26.6, 33.4,  0. ],\n",
       "       [18.4, 28.9,  0. , ..., 20.3, 27. ,  0. ],\n",
       "       [19.4, 37.6,  0. , ..., 28.7, 34.9,  0. ],\n",
       "       ...,\n",
       "       [20.7, 32.8,  0. , ..., 24.8, 32.1,  0. ],\n",
       "       [19.5, 31.8,  0. , ..., 24.8, 29.2,  0. ],\n",
       "       [20.2, 31.7,  0. , ..., 25.4, 31. ,  0. ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RISK_MM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.464770</td>\n",
       "      <td>24.219206</td>\n",
       "      <td>2.130397</td>\n",
       "      <td>5.503135</td>\n",
       "      <td>7.735626</td>\n",
       "      <td>40.877366</td>\n",
       "      <td>15.667228</td>\n",
       "      <td>19.786778</td>\n",
       "      <td>65.874123</td>\n",
       "      <td>49.601985</td>\n",
       "      <td>1017.239505</td>\n",
       "      <td>1014.795580</td>\n",
       "      <td>4.241705</td>\n",
       "      <td>4.326515</td>\n",
       "      <td>18.204961</td>\n",
       "      <td>22.710333</td>\n",
       "      <td>2.346960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.416689</td>\n",
       "      <td>6.970676</td>\n",
       "      <td>7.014822</td>\n",
       "      <td>3.696282</td>\n",
       "      <td>3.758153</td>\n",
       "      <td>13.335232</td>\n",
       "      <td>8.317005</td>\n",
       "      <td>8.510180</td>\n",
       "      <td>18.513289</td>\n",
       "      <td>20.197040</td>\n",
       "      <td>6.909357</td>\n",
       "      <td>6.870892</td>\n",
       "      <td>2.797162</td>\n",
       "      <td>2.647251</td>\n",
       "      <td>6.567991</td>\n",
       "      <td>6.836543</td>\n",
       "      <td>8.731885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6.700000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>980.500000</td>\n",
       "      <td>977.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.600000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1012.700000</td>\n",
       "      <td>1010.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.200000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1017.200000</td>\n",
       "      <td>1014.700000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.400000</td>\n",
       "      <td>29.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1021.800000</td>\n",
       "      <td>1019.400000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.400000</td>\n",
       "      <td>48.100000</td>\n",
       "      <td>206.200000</td>\n",
       "      <td>81.200000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1040.400000</td>\n",
       "      <td>1038.900000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>46.100000</td>\n",
       "      <td>367.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MinTemp       MaxTemp      Rainfall   Evaporation      Sunshine  \\\n",
       "count  56420.000000  56420.000000  56420.000000  56420.000000  56420.000000   \n",
       "mean      13.464770     24.219206      2.130397      5.503135      7.735626   \n",
       "std        6.416689      6.970676      7.014822      3.696282      3.758153   \n",
       "min       -6.700000      4.100000      0.000000      0.000000      0.000000   \n",
       "25%        8.600000     18.700000      0.000000      2.800000      5.000000   \n",
       "50%       13.200000     23.900000      0.000000      5.000000      8.600000   \n",
       "75%       18.400000     29.700000      0.600000      7.400000     10.700000   \n",
       "max       31.400000     48.100000    206.200000     81.200000     14.500000   \n",
       "\n",
       "       WindGustSpeed  WindSpeed9am  WindSpeed3pm   Humidity9am   Humidity3pm  \\\n",
       "count   56420.000000  56420.000000  56420.000000  56420.000000  56420.000000   \n",
       "mean       40.877366     15.667228     19.786778     65.874123     49.601985   \n",
       "std        13.335232      8.317005      8.510180     18.513289     20.197040   \n",
       "min         9.000000      2.000000      2.000000      0.000000      0.000000   \n",
       "25%        31.000000      9.000000     13.000000     55.000000     35.000000   \n",
       "50%        39.000000     15.000000     19.000000     67.000000     50.000000   \n",
       "75%        48.000000     20.000000     26.000000     79.000000     63.000000   \n",
       "max       124.000000     67.000000     76.000000    100.000000    100.000000   \n",
       "\n",
       "        Pressure9am   Pressure3pm      Cloud9am      Cloud3pm       Temp9am  \\\n",
       "count  56420.000000  56420.000000  56420.000000  56420.000000  56420.000000   \n",
       "mean    1017.239505   1014.795580      4.241705      4.326515     18.204961   \n",
       "std        6.909357      6.870892      2.797162      2.647251      6.567991   \n",
       "min      980.500000    977.100000      0.000000      0.000000     -0.700000   \n",
       "25%     1012.700000   1010.100000      1.000000      2.000000     13.100000   \n",
       "50%     1017.200000   1014.700000      5.000000      5.000000     17.800000   \n",
       "75%     1021.800000   1019.400000      7.000000      7.000000     23.300000   \n",
       "max     1040.400000   1038.900000      8.000000      9.000000     39.400000   \n",
       "\n",
       "            Temp3pm       RISK_MM  \n",
       "count  56420.000000  56420.000000  \n",
       "mean      22.710333      2.346960  \n",
       "std        6.836543      8.731885  \n",
       "min        3.700000      0.000000  \n",
       "25%       17.400000      0.000000  \n",
       "50%       22.400000      0.000000  \n",
       "75%       27.900000      0.600000  \n",
       "max       46.100000    367.600000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_num.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Split Data For Cross Validation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split the data into four new datasets, training features, training outcome, test features, \n",
    "# and test outcome. Set the size of the test data to be 20% of the full dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sonar, y_sonar, test_size=0.20, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 1.000000 (0.000000)\n",
      "LDA: 0.880140 (0.002993)\n",
      "KNN: 0.890331 (0.004087)\n",
      "CART: 1.000000 (0.000000)\n",
      "NB: 0.949530 (0.003297)\n",
      "SVM: 0.779710 (0.003849)\n",
      "RF: 0.969758 (0.003376)\n"
     ]
    }
   ],
   "source": [
    "num_folds=10\n",
    "scoring='accuracy'\n",
    "models = []\n",
    "models.append(('LR',  LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART',DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('NB',  GaussianNB()))\n",
    "models.append(('SVM', SVC(random_state=seed)))\n",
    "models.append(('RF',  RandomForestClassifier(max_depth=3, random_state=seed)))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Standardization Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: 0.741544 (0.105022)\n",
      "ScaledLDA: 0.698529 (0.132261)\n",
      "ScaledKNN: 0.795588 (0.069953)\n",
      "ScaledCART: 0.734559 (0.066924)\n",
      "ScaledNB: 0.656985 (0.130658)\n",
      "ScaledSVM: 0.849265 (0.077153)\n",
      "ScaledRF: 0.710294 (0.125731)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART',Pipeline([('Scaler',StandardScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('ScaledNB',Pipeline([('Scaler',StandardScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('ScaledRF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training a  SVM classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No normalization or standartization\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "72.29%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "71.43%\n",
      "\n",
      "##################################################\n",
      "Data after standard scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "98.80%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "83.33%\n",
      "\n",
      "##################################################\n",
      "Data after min-max scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "81.93%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "71.43%\n",
      "\n",
      "##################################################\n",
      "Data after max-abs scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "81.33%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "69.05%\n",
      "\n",
      "##################################################\n",
      "Data after robust scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "96.39%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "80.95%\n",
      "\n",
      "##################################################\n",
      "Data after power transformation (Yeo-Johnson)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "80.95%\n",
      "\n",
      "##################################################\n",
      "Data after quantile transformation (gaussian pdf)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "85.71%\n",
      "\n",
      "##################################################\n",
      "Data after quantile transformation (uniform pdf)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "82.53%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "76.19%\n",
      "\n",
      "##################################################\n",
      "Data after sample-wise L2 normalizing\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "51.20%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "61.90%\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "\n",
    "# std_scale = StandardScaler().fit(X_train)\n",
    "distributions = [\n",
    "    ('Data after standard scaling',\n",
    "        StandardScaler()),\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler()),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler()),\n",
    "    ('Data after robust scaling',\n",
    "        RobustScaler(quantile_range=(25, 75))),\n",
    "    ('Data after power transformation (Yeo-Johnson)',\n",
    "     PowerTransformer(method='yeo-johnson')),\n",
    "#     ('Data after power transformation (Box-Cox)',\n",
    "#      PowerTransformer(method='box-cox')),\n",
    "    ('Data after quantile transformation (gaussian pdf)',\n",
    "        QuantileTransformer(output_distribution='normal')\n",
    "        ),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')\n",
    "        ),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer()),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(\"No normalization or standartization\")\n",
    "svc_scaled = SVC(C=1.5, random_state=seed)\n",
    "fit_std = svc_scaled.fit(X_train, y_train)\n",
    "pred_train_std = svc_scaled.predict(X_train)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "\n",
    "pred_test_std = svc_scaled.predict(X_test)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "\n",
    "\n",
    "print(\"#\"*50)\n",
    "\n",
    "    \n",
    "for name, dist in distributions:\n",
    "    print(name)\n",
    "    std_scale = dist.fit(X_train)\n",
    "    X_train_std = std_scale.transform(X_train)\n",
    "    X_test_std = std_scale.transform(X_test)\n",
    "\n",
    "    # on standardized data\n",
    "    svc_scaled = SVC(C=1.5, random_state=seed)\n",
    "    fit_std = svc_scaled.fit(X_train_std, y_train)\n",
    "    pred_train_std = svc_scaled.predict(X_train_std)\n",
    "\n",
    "    print('\\nPrediction accuracy for the training dataset')\n",
    "    print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "\n",
    "    pred_test_std = svc_scaled.predict(X_test_std)\n",
    "\n",
    "    print('\\nPrediction accuracy for the test dataset')\n",
    "    print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "    \n",
    "    \n",
    "    print(\"#\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(names, resutls, test_scores):\n",
    "    print()\n",
    "    print(\"#\"*30 +\"Results\" + \"#\"*30)\n",
    "    counter = 0\n",
    "    class color:\n",
    "       PURPLE = '\\033[95m'\n",
    "       CYAN = '\\033[96m'\n",
    "       DARKCYAN = '\\033[36m'\n",
    "       BLUE = '\\033[94m'\n",
    "       GREEN = '\\033[92m'\n",
    "       YELLOW = '\\033[93m'\n",
    "       RED = '\\033[91m'\n",
    "       BOLD = '\\033[1m'\n",
    "       UNDERLINE = '\\033[4m'\n",
    "       END = '\\033[0m'\n",
    "\n",
    "\n",
    "    # Get max row\n",
    "    clf_names = set([name.split(\"_\")[1] for name in names])\n",
    "    max_mean = {name:0 for name in clf_names}\n",
    "    max_mean_counter = {name:0 for name in clf_names}\n",
    "    for name,result in zip(names,results):\n",
    "        counter +=1\n",
    "        clf_name = name.split(\"_\")[1]\n",
    "        if result.mean()>max_mean[clf_name]:\n",
    "            max_mean_counter[clf_name] = counter\n",
    "            max_mean[clf_name] = result.mean()\n",
    "\n",
    "    # print max row in BOLD\n",
    "    counter = 0\n",
    "    prev_clf_name = names[0].split(\"_\")[1]\n",
    "    for name,result ,score in zip(names,results,test_scores): \n",
    "        counter +=1\n",
    "        clf_name = name.split(\"_\")[1]\n",
    "        if prev_clf_name != clf_name:\n",
    "            print()\n",
    "            prev_clf_name = clf_name\n",
    "        msg = \"%s: %f (%f) [test_score:%.3f]\" % (name, result.mean(), result.std(), score)\n",
    "        if counter==max_mean_counter[clf_name]:\n",
    "            print(color.BOLD + msg)\n",
    "        else:\n",
    "            print(color.END + msg)\n",
    "            \n",
    "def print_results2(names, results_mean,results_std, test_scores):\n",
    "    print()\n",
    "    print(\"#\"*30 +\"Results\" + \"#\"*30)\n",
    "    class color:\n",
    "       PURPLE = '\\033[95m'\n",
    "       CYAN = '\\033[96m'\n",
    "       DARKCYAN = '\\033[36m'\n",
    "       BLUE = '\\033[94m'\n",
    "       GREEN = '\\033[92m'\n",
    "       YELLOW = '\\033[93m'\n",
    "       RED = '\\033[91m'\n",
    "       BOLD = '\\033[1m'\n",
    "       UNDERLINE = '\\033[4m'\n",
    "       END = '\\033[0m'\n",
    "\n",
    "\n",
    "\n",
    "    # print max row in BOLD\n",
    "    prev_clf_name = names[0].split(\"_\")[1]\n",
    "    for name,mean,std, score in zip(names,results_mean,results_std, test_scores): \n",
    "        clf_name = name.split(\"_\")[1]\n",
    "        if prev_clf_name != clf_name:\n",
    "            print()\n",
    "            prev_clf_name = clf_name\n",
    "        \n",
    "        msg = \"%s: %f (%f) [test_score:%.3f]\" % (name, mean, std, score)\n",
    "        if mean==max(results_mean):\n",
    "            print(color.BOLD + msg)\n",
    "        else:\n",
    "            print(color.END + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LR: 0.753676 (0.113733)\n",
      "Scaled_LR: 0.741544 (0.105022)\n",
      "MinMax_LR: 0.777941 (0.120510)\n",
      "MaxAbsScaler_LR: 0.777941 (0.120510)\n",
      "RobustScaler_LR: 0.735294 (0.076785)\n",
      "QuantileTransformer-Normal_LR: 0.718015 (0.134922)\n",
      "QuantileTransformer-Uniform_LR: 0.808088 (0.116896)\n",
      "PowerTransformer-yeo-johnson_LR: 0.759559 (0.089047)\n",
      "Normalizer_LR: 0.698529 (0.096671)\n",
      "_LR-PCA: 0.753676 (0.113733)\n",
      "Scaled_LR-PCA: 0.752574 (0.146544)\n",
      "MinMax_LR-PCA: 0.752941 (0.145486)\n",
      "MaxAbsScaler_LR-PCA: 0.758824 (0.142239)\n",
      "RobustScaler_LR-PCA: 0.733824 (0.130514)\n",
      "QuantileTransformer-Normal_LR-PCA: 0.741176 (0.155095)\n",
      "QuantileTransformer-Uniform_LR-PCA: 0.788603 (0.117316)\n",
      "PowerTransformer-yeo-johnson_LR-PCA: 0.782353 (0.116780)\n",
      "Normalizer_LR-PCA: 0.631250 (0.068644)\n",
      "_LDA: 0.698529 (0.132261)\n",
      "Scaled_LDA: 0.698529 (0.132261)\n",
      "MinMax_LDA: 0.698529 (0.132261)\n",
      "MaxAbsScaler_LDA: 0.698529 (0.132261)\n",
      "RobustScaler_LDA: 0.698529 (0.132261)\n",
      "QuantileTransformer-Normal_LDA: 0.694485 (0.156079)\n",
      "QuantileTransformer-Uniform_LDA: 0.741912 (0.098387)\n",
      "PowerTransformer-yeo-johnson_LDA: 0.747426 (0.096279)\n",
      "Normalizer_LDA: 0.692279 (0.115010)\n",
      "_LDA-PCA: 0.698529 (0.132261)\n",
      "Scaled_LDA-PCA: 0.740809 (0.142685)\n",
      "MinMax_LDA-PCA: 0.752941 (0.145486)\n",
      "MaxAbsScaler_LDA-PCA: 0.758824 (0.142239)\n",
      "RobustScaler_LDA-PCA: 0.733824 (0.121203)\n",
      "QuantileTransformer-Normal_LDA-PCA: 0.753309 (0.156295)\n",
      "QuantileTransformer-Uniform_LDA-PCA: 0.782721 (0.099230)\n",
      "PowerTransformer-yeo-johnson_LDA-PCA: 0.782353 (0.116780)\n",
      "Normalizer_LDA-PCA: 0.642279 (0.121469)\n",
      "_KNN: 0.753309 (0.080505)\n",
      "Scaled_KNN: 0.795588 (0.069953)\n",
      "MinMax_KNN: 0.813235 (0.074093)\n",
      "MaxAbsScaler_KNN: 0.807721 (0.063531)\n",
      "RobustScaler_KNN: 0.759559 (0.064127)\n",
      "QuantileTransformer-Normal_KNN: 0.782721 (0.071827)\n",
      "QuantileTransformer-Uniform_KNN: 0.788603 (0.111261)\n",
      "PowerTransformer-yeo-johnson_KNN: 0.812868 (0.103810)\n",
      "Normalizer_KNN: 0.765441 (0.084636)\n",
      "_KNN-PCA: 0.753309 (0.080505)\n",
      "Scaled_KNN-PCA: 0.752941 (0.078090)\n",
      "MinMax_KNN-PCA: 0.759191 (0.101143)\n",
      "MaxAbsScaler_KNN-PCA: 0.776838 (0.111961)\n",
      "RobustScaler_KNN-PCA: 0.757721 (0.134746)\n",
      "QuantileTransformer-Normal_KNN-PCA: 0.740809 (0.102866)\n",
      "QuantileTransformer-Uniform_KNN-PCA: 0.739706 (0.089932)\n",
      "PowerTransformer-yeo-johnson_KNN-PCA: 0.776838 (0.091559)\n",
      "Normalizer_KNN-PCA: 0.741544 (0.137188)\n",
      "_CART: 0.734559 (0.066924)\n",
      "Scaled_CART: 0.734559 (0.066924)\n",
      "MinMax_CART: 0.734559 (0.066924)\n",
      "MaxAbsScaler_CART: 0.734559 (0.066924)\n",
      "RobustScaler_CART: 0.734559 (0.066924)\n",
      "QuantileTransformer-Normal_CART: 0.734559 (0.066924)\n",
      "QuantileTransformer-Uniform_CART: 0.740441 (0.066730)\n",
      "PowerTransformer-yeo-johnson_CART: 0.728676 (0.055240)\n",
      "Normalizer_CART: 0.715809 (0.121513)\n",
      "_CART-PCA: 0.734559 (0.066924)\n",
      "Scaled_CART-PCA: 0.691544 (0.138973)\n",
      "MinMax_CART-PCA: 0.741544 (0.123214)\n",
      "MaxAbsScaler_CART-PCA: 0.716544 (0.131713)\n",
      "RobustScaler_CART-PCA: 0.715441 (0.123840)\n",
      "QuantileTransformer-Normal_CART-PCA: 0.705882 (0.083238)\n",
      "QuantileTransformer-Uniform_CART-PCA: 0.709926 (0.080404)\n",
      "PowerTransformer-yeo-johnson_CART-PCA: 0.710294 (0.125645)\n",
      "Normalizer_CART-PCA: 0.571324 (0.073404)\n",
      "_NB: 0.656985 (0.130658)\n",
      "Scaled_NB: 0.656985 (0.130658)\n",
      "MinMax_NB: 0.656985 (0.130658)\n",
      "MaxAbsScaler_NB: 0.656985 (0.130658)\n",
      "RobustScaler_NB: 0.656985 (0.130658)\n",
      "QuantileTransformer-Normal_NB: 0.752206 (0.118667)\n",
      "QuantileTransformer-Uniform_NB: 0.752206 (0.111090)\n",
      "PowerTransformer-yeo-johnson_NB: 0.751838 (0.136824)\n",
      "Normalizer_NB: 0.662132 (0.154634)\n",
      "_NB-PCA: 0.656985 (0.130658)\n",
      "Scaled_NB-PCA: 0.752206 (0.129808)\n",
      "MinMax_NB-PCA: 0.752574 (0.140795)\n",
      "MaxAbsScaler_NB-PCA: 0.752574 (0.140795)\n",
      "RobustScaler_NB-PCA: 0.661397 (0.133456)\n",
      "QuantileTransformer-Normal_NB-PCA: 0.723162 (0.165082)\n",
      "QuantileTransformer-Uniform_NB-PCA: 0.734926 (0.163150)\n",
      "PowerTransformer-yeo-johnson_NB-PCA: 0.758456 (0.156369)\n",
      "Normalizer_NB-PCA: 0.638235 (0.066017)\n",
      "_SVM: 0.608456 (0.115809)\n",
      "Scaled_SVM: 0.849265 (0.077153)\n",
      "MinMax_SVM: 0.711397 (0.086193)\n",
      "MaxAbsScaler_SVM: 0.705147 (0.080728)\n",
      "RobustScaler_SVM: 0.776471 (0.132473)\n",
      "QuantileTransformer-Normal_SVM: 0.831985 (0.094193)\n",
      "QuantileTransformer-Uniform_SVM: 0.771691 (0.114272)\n",
      "PowerTransformer-yeo-johnson_SVM: 0.813603 (0.114869)\n",
      "Normalizer_SVM: 0.524265 (0.102957)\n",
      "_SVM-PCA: 0.608456 (0.115809)\n",
      "Scaled_SVM-PCA: 0.776471 (0.068659)\n",
      "MinMax_SVM-PCA: 0.758824 (0.117580)\n",
      "MaxAbsScaler_SVM-PCA: 0.758824 (0.128815)\n",
      "RobustScaler_SVM-PCA: 0.787868 (0.111864)\n",
      "QuantileTransformer-Normal_SVM-PCA: 0.729044 (0.087328)\n",
      "QuantileTransformer-Uniform_SVM-PCA: 0.746691 (0.119114)\n",
      "PowerTransformer-yeo-johnson_SVM-PCA: 0.740074 (0.075579)\n",
      "Normalizer_SVM-PCA: 0.589338 (0.064648)\n",
      "_RF: 0.710294 (0.125731)\n",
      "Scaled_RF: 0.710294 (0.125731)\n",
      "MinMax_RF: 0.710294 (0.125731)\n",
      "MaxAbsScaler_RF: 0.710294 (0.125731)\n",
      "RobustScaler_RF: 0.710294 (0.125731)\n",
      "QuantileTransformer-Normal_RF: 0.716544 (0.134977)\n",
      "QuantileTransformer-Uniform_RF: 0.716544 (0.134977)\n",
      "PowerTransformer-yeo-johnson_RF: 0.710294 (0.125731)\n",
      "Normalizer_RF: 0.722794 (0.116946)\n",
      "_RF-PCA: 0.710294 (0.125731)\n",
      "Scaled_RF-PCA: 0.674632 (0.120421)\n",
      "MinMax_RF-PCA: 0.728309 (0.117618)\n",
      "MaxAbsScaler_RF-PCA: 0.728676 (0.123988)\n",
      "RobustScaler_RF-PCA: 0.703309 (0.148895)\n",
      "QuantileTransformer-Normal_RF-PCA: 0.765809 (0.104544)\n",
      "QuantileTransformer-Uniform_RF-PCA: 0.668382 (0.137274)\n",
      "PowerTransformer-yeo-johnson_RF-PCA: 0.710294 (0.127673)\n",
      "Normalizer_RF-PCA: 0.643015 (0.085898)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_MLP: 0.778309 (0.125173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_MLP: 0.819853 (0.084415)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_MLP: 0.766544 (0.143234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_MLP: 0.766544 (0.143234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_MLP: 0.807721 (0.087606)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_MLP: 0.808456 (0.117215)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_MLP: 0.807721 (0.105522)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_MLP: 0.838603 (0.110764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_MLP: 0.723529 (0.131778)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_MLP-PCA: 0.778309 (0.125173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_MLP-PCA: 0.776838 (0.090579)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_MLP-PCA: 0.747059 (0.110292)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_MLP-PCA: 0.740809 (0.107743)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_MLP-PCA: 0.770588 (0.089062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_MLP-PCA: 0.728676 (0.119684)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_MLP-PCA: 0.776103 (0.103706)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_MLP-PCA: 0.770221 (0.122909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_MLP-PCA: 0.618750 (0.097410)\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_LR: 0.753676 (0.113733) [test_score:0.714]\n",
      "\u001b[0mScaled_LR: 0.741544 (0.105022) [test_score:0.833]\n",
      "\u001b[0mMinMax_LR: 0.777941 (0.120510) [test_score:0.762]\n",
      "\u001b[0mMaxAbsScaler_LR: 0.777941 (0.120510) [test_score:0.762]\n",
      "\u001b[0mRobustScaler_LR: 0.735294 (0.076785) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Normal_LR: 0.718015 (0.134922) [test_score:0.810]\n",
      "\u001b[1mQuantileTransformer-Uniform_LR: 0.808088 (0.116896) [test_score:0.786]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LR: 0.759559 (0.089047) [test_score:0.833]\n",
      "\u001b[0mNormalizer_LR: 0.698529 (0.096671) [test_score:0.690]\n",
      "\n",
      "\u001b[0m_LR-PCA: 0.753676 (0.113733) [test_score:0.714]\n",
      "\u001b[0mScaled_LR-PCA: 0.752574 (0.146544) [test_score:0.738]\n",
      "\u001b[0mMinMax_LR-PCA: 0.752941 (0.145486) [test_score:0.714]\n",
      "\u001b[0mMaxAbsScaler_LR-PCA: 0.758824 (0.142239) [test_score:0.714]\n",
      "\u001b[0mRobustScaler_LR-PCA: 0.733824 (0.130514) [test_score:0.738]\n",
      "\u001b[0mQuantileTransformer-Normal_LR-PCA: 0.741176 (0.155095) [test_score:0.714]\n",
      "\u001b[1mQuantileTransformer-Uniform_LR-PCA: 0.788603 (0.117316) [test_score:0.738]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LR-PCA: 0.782353 (0.116780) [test_score:0.714]\n",
      "\u001b[0mNormalizer_LR-PCA: 0.631250 (0.068644) [test_score:0.571]\n",
      "\n",
      "\u001b[0m_LDA: 0.698529 (0.132261) [test_score:0.833]\n",
      "\u001b[0mScaled_LDA: 0.698529 (0.132261) [test_score:0.833]\n",
      "\u001b[0mMinMax_LDA: 0.698529 (0.132261) [test_score:0.833]\n",
      "\u001b[0mMaxAbsScaler_LDA: 0.698529 (0.132261) [test_score:0.833]\n",
      "\u001b[0mRobustScaler_LDA: 0.698529 (0.132261) [test_score:0.833]\n",
      "\u001b[0mQuantileTransformer-Normal_LDA: 0.694485 (0.156079) [test_score:0.738]\n",
      "\u001b[0mQuantileTransformer-Uniform_LDA: 0.741912 (0.098387) [test_score:0.786]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_LDA: 0.747426 (0.096279) [test_score:0.786]\n",
      "\u001b[0mNormalizer_LDA: 0.692279 (0.115010) [test_score:0.833]\n",
      "\n",
      "\u001b[0m_LDA-PCA: 0.698529 (0.132261) [test_score:0.833]\n",
      "\u001b[0mScaled_LDA-PCA: 0.740809 (0.142685) [test_score:0.714]\n",
      "\u001b[0mMinMax_LDA-PCA: 0.752941 (0.145486) [test_score:0.714]\n",
      "\u001b[0mMaxAbsScaler_LDA-PCA: 0.758824 (0.142239) [test_score:0.714]\n",
      "\u001b[0mRobustScaler_LDA-PCA: 0.733824 (0.121203) [test_score:0.690]\n",
      "\u001b[0mQuantileTransformer-Normal_LDA-PCA: 0.753309 (0.156295) [test_score:0.714]\n",
      "\u001b[1mQuantileTransformer-Uniform_LDA-PCA: 0.782721 (0.099230) [test_score:0.738]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LDA-PCA: 0.782353 (0.116780) [test_score:0.690]\n",
      "\u001b[0mNormalizer_LDA-PCA: 0.642279 (0.121469) [test_score:0.571]\n",
      "\n",
      "\u001b[0m_KNN: 0.753309 (0.080505) [test_score:0.762]\n",
      "\u001b[0mScaled_KNN: 0.795588 (0.069953) [test_score:0.833]\n",
      "\u001b[1mMinMax_KNN: 0.813235 (0.074093) [test_score:0.786]\n",
      "\u001b[0mMaxAbsScaler_KNN: 0.807721 (0.063531) [test_score:0.762]\n",
      "\u001b[0mRobustScaler_KNN: 0.759559 (0.064127) [test_score:0.833]\n",
      "\u001b[0mQuantileTransformer-Normal_KNN: 0.782721 (0.071827) [test_score:0.833]\n",
      "\u001b[0mQuantileTransformer-Uniform_KNN: 0.788603 (0.111261) [test_score:0.905]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_KNN: 0.812868 (0.103810) [test_score:0.786]\n",
      "\u001b[0mNormalizer_KNN: 0.765441 (0.084636) [test_score:0.714]\n",
      "\n",
      "\u001b[0m_KNN-PCA: 0.753309 (0.080505) [test_score:0.762]\n",
      "\u001b[0mScaled_KNN-PCA: 0.752941 (0.078090) [test_score:0.786]\n",
      "\u001b[0mMinMax_KNN-PCA: 0.759191 (0.101143) [test_score:0.810]\n",
      "\u001b[0mMaxAbsScaler_KNN-PCA: 0.776838 (0.111961) [test_score:0.810]\n",
      "\u001b[0mRobustScaler_KNN-PCA: 0.757721 (0.134746) [test_score:0.714]\n",
      "\u001b[0mQuantileTransformer-Normal_KNN-PCA: 0.740809 (0.102866) [test_score:0.786]\n",
      "\u001b[0mQuantileTransformer-Uniform_KNN-PCA: 0.739706 (0.089932) [test_score:0.833]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_KNN-PCA: 0.776838 (0.091559) [test_score:0.810]\n",
      "\u001b[0mNormalizer_KNN-PCA: 0.741544 (0.137188) [test_score:0.690]\n",
      "\n",
      "\u001b[0m_CART: 0.734559 (0.066924) [test_score:0.810]\n",
      "\u001b[0mScaled_CART: 0.734559 (0.066924) [test_score:0.810]\n",
      "\u001b[0mMinMax_CART: 0.734559 (0.066924) [test_score:0.810]\n",
      "\u001b[0mMaxAbsScaler_CART: 0.734559 (0.066924) [test_score:0.810]\n",
      "\u001b[0mRobustScaler_CART: 0.734559 (0.066924) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Normal_CART: 0.734559 (0.066924) [test_score:0.810]\n",
      "\u001b[1mQuantileTransformer-Uniform_CART: 0.740441 (0.066730) [test_score:0.810]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_CART: 0.728676 (0.055240) [test_score:0.810]\n",
      "\u001b[0mNormalizer_CART: 0.715809 (0.121513) [test_score:0.762]\n",
      "\n",
      "\u001b[0m_CART-PCA: 0.734559 (0.066924) [test_score:0.810]\n",
      "\u001b[0mScaled_CART-PCA: 0.691544 (0.138973) [test_score:0.762]\n",
      "\u001b[1mMinMax_CART-PCA: 0.741544 (0.123214) [test_score:0.762]\n",
      "\u001b[0mMaxAbsScaler_CART-PCA: 0.716544 (0.131713) [test_score:0.762]\n",
      "\u001b[0mRobustScaler_CART-PCA: 0.715441 (0.123840) [test_score:0.643]\n",
      "\u001b[0mQuantileTransformer-Normal_CART-PCA: 0.705882 (0.083238) [test_score:0.762]\n",
      "\u001b[0mQuantileTransformer-Uniform_CART-PCA: 0.709926 (0.080404) [test_score:0.810]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_CART-PCA: 0.710294 (0.125645) [test_score:0.810]\n",
      "\u001b[0mNormalizer_CART-PCA: 0.571324 (0.073404) [test_score:0.762]\n",
      "\n",
      "\u001b[0m_NB: 0.656985 (0.130658) [test_score:0.690]\n",
      "\u001b[0mScaled_NB: 0.656985 (0.130658) [test_score:0.690]\n",
      "\u001b[0mMinMax_NB: 0.656985 (0.130658) [test_score:0.690]\n",
      "\u001b[0mMaxAbsScaler_NB: 0.656985 (0.130658) [test_score:0.690]\n",
      "\u001b[0mRobustScaler_NB: 0.656985 (0.130658) [test_score:0.690]\n",
      "\u001b[1mQuantileTransformer-Normal_NB: 0.752206 (0.118667) [test_score:0.762]\n",
      "\u001b[0mQuantileTransformer-Uniform_NB: 0.752206 (0.111090) [test_score:0.786]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_NB: 0.751838 (0.136824) [test_score:0.762]\n",
      "\u001b[0mNormalizer_NB: 0.662132 (0.154634) [test_score:0.643]\n",
      "\n",
      "\u001b[0m_NB-PCA: 0.656985 (0.130658) [test_score:0.690]\n",
      "\u001b[0mScaled_NB-PCA: 0.752206 (0.129808) [test_score:0.786]\n",
      "\u001b[0mMinMax_NB-PCA: 0.752574 (0.140795) [test_score:0.762]\n",
      "\u001b[0mMaxAbsScaler_NB-PCA: 0.752574 (0.140795) [test_score:0.762]\n",
      "\u001b[0mRobustScaler_NB-PCA: 0.661397 (0.133456) [test_score:0.643]\n",
      "\u001b[0mQuantileTransformer-Normal_NB-PCA: 0.723162 (0.165082) [test_score:0.714]\n",
      "\u001b[0mQuantileTransformer-Uniform_NB-PCA: 0.734926 (0.163150) [test_score:0.762]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_NB-PCA: 0.758456 (0.156369) [test_score:0.762]\n",
      "\u001b[0mNormalizer_NB-PCA: 0.638235 (0.066017) [test_score:0.571]\n",
      "\n",
      "\u001b[0m_SVM: 0.608456 (0.115809) [test_score:0.738]\n",
      "\u001b[1mScaled_SVM: 0.849265 (0.077153) [test_score:0.833]\n",
      "\u001b[0mMinMax_SVM: 0.711397 (0.086193) [test_score:0.714]\n",
      "\u001b[0mMaxAbsScaler_SVM: 0.705147 (0.080728) [test_score:0.690]\n",
      "\u001b[0mRobustScaler_SVM: 0.776471 (0.132473) [test_score:0.786]\n",
      "\u001b[0mQuantileTransformer-Normal_SVM: 0.831985 (0.094193) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Uniform_SVM: 0.771691 (0.114272) [test_score:0.786]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_SVM: 0.813603 (0.114869) [test_score:0.833]\n",
      "\u001b[0mNormalizer_SVM: 0.524265 (0.102957) [test_score:0.619]\n",
      "\n",
      "\u001b[0m_SVM-PCA: 0.608456 (0.115809) [test_score:0.738]\n",
      "\u001b[0mScaled_SVM-PCA: 0.776471 (0.068659) [test_score:0.738]\n",
      "\u001b[0mMinMax_SVM-PCA: 0.758824 (0.117580) [test_score:0.762]\n",
      "\u001b[0mMaxAbsScaler_SVM-PCA: 0.758824 (0.128815) [test_score:0.762]\n",
      "\u001b[1mRobustScaler_SVM-PCA: 0.787868 (0.111864) [test_score:0.738]\n",
      "\u001b[0mQuantileTransformer-Normal_SVM-PCA: 0.729044 (0.087328) [test_score:0.762]\n",
      "\u001b[0mQuantileTransformer-Uniform_SVM-PCA: 0.746691 (0.119114) [test_score:0.833]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_SVM-PCA: 0.740074 (0.075579) [test_score:0.857]\n",
      "\u001b[0mNormalizer_SVM-PCA: 0.589338 (0.064648) [test_score:0.595]\n",
      "\n",
      "\u001b[0m_RF: 0.710294 (0.125731) [test_score:0.810]\n",
      "\u001b[0mScaled_RF: 0.710294 (0.125731) [test_score:0.810]\n",
      "\u001b[0mMinMax_RF: 0.710294 (0.125731) [test_score:0.810]\n",
      "\u001b[0mMaxAbsScaler_RF: 0.710294 (0.125731) [test_score:0.810]\n",
      "\u001b[0mRobustScaler_RF: 0.710294 (0.125731) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Normal_RF: 0.716544 (0.134977) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Uniform_RF: 0.716544 (0.134977) [test_score:0.810]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_RF: 0.710294 (0.125731) [test_score:0.810]\n",
      "\u001b[1mNormalizer_RF: 0.722794 (0.116946) [test_score:0.762]\n",
      "\n",
      "\u001b[0m_RF-PCA: 0.710294 (0.125731) [test_score:0.810]\n",
      "\u001b[0mScaled_RF-PCA: 0.674632 (0.120421) [test_score:0.762]\n",
      "\u001b[0mMinMax_RF-PCA: 0.728309 (0.117618) [test_score:0.738]\n",
      "\u001b[0mMaxAbsScaler_RF-PCA: 0.728676 (0.123988) [test_score:0.810]\n",
      "\u001b[0mRobustScaler_RF-PCA: 0.703309 (0.148895) [test_score:0.786]\n",
      "\u001b[1mQuantileTransformer-Normal_RF-PCA: 0.765809 (0.104544) [test_score:0.833]\n",
      "\u001b[0mQuantileTransformer-Uniform_RF-PCA: 0.668382 (0.137274) [test_score:0.810]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_RF-PCA: 0.710294 (0.127673) [test_score:0.762]\n",
      "\u001b[0mNormalizer_RF-PCA: 0.643015 (0.085898) [test_score:0.643]\n",
      "\n",
      "\u001b[0m_MLP: 0.778309 (0.125173) [test_score:0.833]\n",
      "\u001b[0mScaled_MLP: 0.819853 (0.084415) [test_score:0.881]\n",
      "\u001b[0mMinMax_MLP: 0.766544 (0.143234) [test_score:0.810]\n",
      "\u001b[0mMaxAbsScaler_MLP: 0.766544 (0.143234) [test_score:0.810]\n",
      "\u001b[0mRobustScaler_MLP: 0.807721 (0.087606) [test_score:0.881]\n",
      "\u001b[0mQuantileTransformer-Normal_MLP: 0.808456 (0.117215) [test_score:0.833]\n",
      "\u001b[0mQuantileTransformer-Uniform_MLP: 0.807721 (0.105522) [test_score:0.810]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_MLP: 0.838603 (0.110764) [test_score:0.881]\n",
      "\u001b[0mNormalizer_MLP: 0.723529 (0.131778) [test_score:0.762]\n",
      "\n",
      "\u001b[1m_MLP-PCA: 0.778309 (0.125173) [test_score:0.833]\n",
      "\u001b[0mScaled_MLP-PCA: 0.776838 (0.090579) [test_score:0.762]\n",
      "\u001b[0mMinMax_MLP-PCA: 0.747059 (0.110292) [test_score:0.786]\n",
      "\u001b[0mMaxAbsScaler_MLP-PCA: 0.740809 (0.107743) [test_score:0.786]\n",
      "\u001b[0mRobustScaler_MLP-PCA: 0.770588 (0.089062) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Normal_MLP-PCA: 0.728676 (0.119684) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Uniform_MLP-PCA: 0.776103 (0.103706) [test_score:0.786]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_MLP-PCA: 0.770221 (0.122909) [test_score:0.786]\n",
      "\u001b[0mNormalizer_MLP-PCA: 0.618750 (0.097410) [test_score:0.571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_LR',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR',Pipeline([('Scaler',MinMaxScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR',Pipeline([('Scaler',MaxAbsScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR',Pipeline([('Scaler',RobustScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LR',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LR',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LR',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR',Pipeline([('Scaler',Normalizer()),('LR',LogisticRegression())])))\n",
    "\n",
    "pipelines.append(('_LR-PCA',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LR-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LR-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LR-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_LDA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA',Pipeline([('Scaler',MinMaxScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA',Pipeline([('Scaler',MaxAbsScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA',Pipeline([('Scaler',RobustScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LDA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LDA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LDA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA',Pipeline([('Scaler',Normalizer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "pipelines.append(('_LDA-PCA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LDA-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LDA-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LDA-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_KNN',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN',Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN',Pipeline([('Scaler',MaxAbsScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN',Pipeline([('Scaler',RobustScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_KNN',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_KNN',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_KNN',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN',Pipeline([('Scaler',Normalizer()),('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "pipelines.append(('_KNN-PCA',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_KNN-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_KNN-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_KNN-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_CART',Pipeline([('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_CART',Pipeline([('Scaler',StandardScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_CART',Pipeline([('Scaler',MinMaxScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_CART',Pipeline([('Scaler',MaxAbsScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_CART',Pipeline([('Scaler',RobustScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_CART',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_CART',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_CART',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_CART',Pipeline([('Scaler',Normalizer()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_CART-PCA',Pipeline([('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_CART-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_CART-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_CART-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_CART-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_CART-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_CART-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_CART-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_CART-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_NB',Pipeline([('NB',GaussianNB())])))\n",
    "pipelines.append(('Scaled_NB',Pipeline([('Scaler',StandardScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('MinMax_NB',Pipeline([('Scaler',MinMaxScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('MaxAbsScaler_NB',Pipeline([('Scaler',MaxAbsScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('RobustScaler_NB',Pipeline([('Scaler',RobustScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_NB',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_NB',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('NB',GaussianNB())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_NB',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('NB',GaussianNB())])))\n",
    "pipelines.append(('Normalizer_NB',Pipeline([('Scaler',Normalizer()),('NB',GaussianNB())])))\n",
    "\n",
    "pipelines.append(('_NB-PCA',Pipeline([('NB',GaussianNB())])))\n",
    "pipelines.append(('Scaled_NB-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('MinMax_NB-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('MaxAbsScaler_NB-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('RobustScaler_NB-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_NB-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_NB-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_NB-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('Normalizer_NB-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('NB',GaussianNB())])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_SVM' ,Pipeline([('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM',Pipeline([('Scaler',MinMaxScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM',Pipeline([('Scaler',MaxAbsScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM',Pipeline([('Scaler',RobustScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_SVM',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_SVM',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_SVM',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM',Pipeline([('Scaler',Normalizer()),('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_SVM-PCA',Pipeline([('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_SVM-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_SVM-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_SVM-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_RF' ,Pipeline([('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF',Pipeline([('Scaler',MinMaxScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF',Pipeline([('Scaler',MaxAbsScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF',Pipeline([('Scaler',RobustScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_RF',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_RF',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_RF',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF',Pipeline([('Scaler',Normalizer()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_RF-PCA',Pipeline([('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_RF-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_RF-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_RF-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "pipelines.append(('_MLP',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP',Pipeline([('Scaler',StandardScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP',Pipeline([('Scaler',MinMaxScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP',Pipeline([('Scaler',MaxAbsScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP',Pipeline([('Scaler',RobustScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_MLP',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_MLP',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_MLP',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP',Pipeline([('Scaler',Normalizer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_MLP-PCA',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_MLP-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_MLP-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_MLP-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "test_scores = []\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "    # fit on train and predict on test\n",
    "    model.fit(X_train,y_train)\n",
    "    test_scores.append(model.score(X_test,y_test))\n",
    "    \n",
    "print_results(names, results, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune parameters and then check Normalization and Standartization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1539 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 5193 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   28.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1516 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4764 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   23.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 2876 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   26.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1836 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 4678 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   28.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1316 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3346 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6176 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   42.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_RF: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:   48.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  3.7min finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_RF: 0.765060 (0.103615)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 4012 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   30.7s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_RF: 0.789157 (0.108115)\n",
      "Test score 0.7380952380952381\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 4012 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   34.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_RF-PCA: 0.759036 (0.094511)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 612 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1830 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3528 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 5718 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   34.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_RF-PCA: 0.722892 (0.095618)\n",
      "Test score 0.7142857142857143\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 4012 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   28.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_RF-PCA: 0.759036 (0.108783)\n",
      "Test score 0.7857142857142857\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4012 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   27.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_RF-PCA: 0.777108 (0.131756)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2680 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4944 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   41.9s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_RF-PCA: 0.777108 (0.143657)\n",
      "Test score 0.7857142857142857\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   42.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3922 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5136 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  3.6min finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_RF-PCA: 0.728916 (0.106791)\n",
      "Test score 0.7857142857142857\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1836 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 4678 tasks      | elapsed:   19.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_RF-PCA: 0.674699 (0.066384)\n",
      "Test score 0.7619047619047619\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_RF: 0.759036 (0.094511)\n",
      "\u001b[0mScaled_RF: 0.759036 (0.094511)\n",
      "\u001b[0mMinMax_RF: 0.759036 (0.094511)\n",
      "\u001b[0mMaxAbsScaler_RF: 0.759036 (0.094511)\n",
      "\u001b[0mRobustScaler_RF: 0.759036 (0.094511)\n",
      "\u001b[0mQuantileTransformer_RF: 0.765060 (0.103615)\n",
      "\u001b[1mNormalizer_RF: 0.789157 (0.108115)\n",
      "\n",
      "\u001b[0m_RF-PCA: 0.759036 (0.094511)\n",
      "\u001b[0mScaled_RF-PCA: 0.722892 (0.095618)\n",
      "\u001b[0mMinMax_RF-PCA: 0.759036 (0.108783)\n",
      "\u001b[0mMaxAbsScaler_RF-PCA: 0.777108 (0.131756)\n",
      "\u001b[0mRobustScaler_RF-PCA: 0.777108 (0.143657)\n",
      "\u001b[0mQuantileTransformer_RF-PCA: 0.728916 (0.106791)\n",
      "\u001b[0mNormalizer_RF-PCA: 0.674699 (0.066384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:   27.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 20, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Create the random grid\n",
    "random_grid = {'RF__n_estimators': n_estimators,\n",
    "               'RF__max_features': max_features,\n",
    "               'RF__max_depth': max_depth,\n",
    "               'RF__min_samples_split': min_samples_split,\n",
    "               'RF__min_samples_leaf': min_samples_leaf,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_RF' ,Pipeline([('RF' , RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF',Pipeline([('Scaler',MinMaxScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF',Pipeline([('Scaler',MaxAbsScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF',Pipeline([('Scaler',RobustScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_RF',Pipeline([('Scaler',QuantileTransformer()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF',Pipeline([('Scaler',Normalizer()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_RF-PCA',Pipeline([('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_RF-PCA',Pipeline([('Scaler',QuantileTransformer()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "        \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_KNN: 0.855422 (0.047806)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_KNN: 0.825301 (0.094589)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_KNN: 0.843373 (0.061421)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_KNN: 0.843373 (0.061421)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 144 out of 320 | elapsed:    0.4s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_KNN: 0.837349 (0.073886)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    7.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_KNN: 0.813253 (0.099759)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    7.9s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_KNN: 0.891566 (0.050710)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    8.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_KNN: 0.891566 (0.058800)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_KNN: 0.801205 (0.091692)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_KNN-PCA: 0.855422 (0.047806)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_KNN-PCA: 0.819277 (0.077372)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_KNN-PCA: 0.783133 (0.135372)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_KNN-PCA: 0.783133 (0.129690)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 144 out of 320 | elapsed:    0.5s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    1.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_KNN-PCA: 0.765060 (0.086233)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    8.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_KNN-PCA: 0.734940 (0.121702)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    9.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_KNN-PCA: 0.765060 (0.115540)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   10.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_KNN-PCA: 0.759036 (0.108579)\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "Normalizer_KNN-PCA: 0.716867 (0.068914)\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_KNN: 0.855422 (0.047806) [test_score:0.857]\n",
      "\u001b[0mScaled_KNN: 0.825301 (0.094589) [test_score:0.952]\n",
      "\u001b[0mMinMax_KNN: 0.843373 (0.061421) [test_score:0.857]\n",
      "\u001b[0mMaxAbsScaler_KNN: 0.843373 (0.061421) [test_score:0.857]\n",
      "\u001b[0mRobustScaler_KNN: 0.837349 (0.073886) [test_score:0.929]\n",
      "\u001b[0mQuantileTransformer-Normal_KNN: 0.813253 (0.099759) [test_score:0.881]\n",
      "\u001b[1mQuantileTransformer-Uniform_KNN: 0.891566 (0.050710) [test_score:0.905]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_KNN: 0.891566 (0.058800) [test_score:0.905]\n",
      "\u001b[0mNormalizer_KNN: 0.801205 (0.091692) [test_score:0.881]\n",
      "\n",
      "\u001b[0m_KNN-PCA: 0.855422 (0.047806) [test_score:0.857]\n",
      "\u001b[0mScaled_KNN-PCA: 0.819277 (0.077372) [test_score:0.738]\n",
      "\u001b[0mMinMax_KNN-PCA: 0.783133 (0.135372) [test_score:0.762]\n",
      "\u001b[0mMaxAbsScaler_KNN-PCA: 0.783133 (0.129690) [test_score:0.786]\n",
      "\u001b[0mRobustScaler_KNN-PCA: 0.765060 (0.086233) [test_score:0.690]\n",
      "\u001b[0mQuantileTransformer-Normal_KNN-PCA: 0.734940 (0.121702) [test_score:0.786]\n",
      "\u001b[0mQuantileTransformer-Uniform_KNN-PCA: 0.765060 (0.115540) [test_score:0.833]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_KNN-PCA: 0.759036 (0.108579) [test_score:0.833]\n",
      "\u001b[0mNormalizer_KNN-PCA: 0.716867 (0.068914) [test_score:0.786]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "n_neighbors  = [int(x) for x in np.linspace(start = 1, stop = 20, num = 2)]\n",
    "weights  = [\"uniform\",\"distance\"]\n",
    "algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "leaf_size =[int(x) for x in np.linspace(start = 5, stop = 50, num = 2)]\n",
    "p =[int(x) for x in np.linspace(start = 1, stop = 4, num = 1)]\n",
    "# Create the random grid\n",
    "random_grid = {'KNN__n_neighbors': n_neighbors,\n",
    "               'KNN__weights': weights,\n",
    "               'KNN__algorithm': algorithm,\n",
    "               'KNN__leaf_size': leaf_size,\n",
    "               'KNN__p': p,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_KNN',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN',Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN',Pipeline([('Scaler',MaxAbsScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN',Pipeline([('Scaler',RobustScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_KNN',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_KNN',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_KNN',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN',Pipeline([('Scaler',Normalizer()),('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "pipelines.append(('_KNN-PCA',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_KNN-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_KNN-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_KNN-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SVM: 0.783133 (0.114643)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 184 out of 400 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_SVM: 0.855422 (0.072632)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_SVM: 0.783133 (0.134634)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_SVM: 0.783133 (0.097068)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    1.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_SVM: 0.801205 (0.089616)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   10.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_SVM: 0.855422 (0.080169)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    9.9s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_SVM: 0.813253 (0.107352)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   10.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_SVM: 0.867470 (0.096210)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_SVM: 0.710843 (0.139746)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 184 out of 400 | elapsed:    0.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SVM-PCA: 0.783133 (0.114643)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    1.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_SVM-PCA: 0.783133 (0.068430)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_SVM-PCA: 0.777108 (0.109443)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_SVM-PCA: 0.771084 (0.129133)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    1.3s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_SVM-PCA: 0.795181 (0.108223)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   10.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_SVM-PCA: 0.734940 (0.086618)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    9.9s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_SVM-PCA: 0.789157 (0.099026)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   11.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_SVM-PCA: 0.759036 (0.115976)\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n",
      "Normalizer_SVM-PCA: 0.632530 (0.088912)\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_SVM: 0.783133 (0.114643) [test_score:0.738]\n",
      "\u001b[0mScaled_SVM: 0.855422 (0.072632) [test_score:0.833]\n",
      "\u001b[0mMinMax_SVM: 0.783133 (0.134634) [test_score:0.738]\n",
      "\u001b[0mMaxAbsScaler_SVM: 0.783133 (0.097068) [test_score:0.762]\n",
      "\u001b[0mRobustScaler_SVM: 0.801205 (0.089616) [test_score:0.810]\n",
      "\u001b[0mQuantileTransformer-Normal_SVM: 0.855422 (0.080169) [test_score:0.857]\n",
      "\u001b[0mQuantileTransformer-Uniform_SVM: 0.813253 (0.107352) [test_score:0.762]\n",
      "\u001b[1mPowerTransformer-yeo-johnson_SVM: 0.867470 (0.096210) [test_score:0.881]\n",
      "\u001b[0mNormalizer_SVM: 0.710843 (0.139746) [test_score:0.714]\n",
      "\n",
      "\u001b[0m_SVM-PCA: 0.783133 (0.114643) [test_score:0.738]\n",
      "\u001b[0mScaled_SVM-PCA: 0.783133 (0.068430) [test_score:0.738]\n",
      "\u001b[0mMinMax_SVM-PCA: 0.777108 (0.109443) [test_score:0.690]\n",
      "\u001b[0mMaxAbsScaler_SVM-PCA: 0.771084 (0.129133) [test_score:0.714]\n",
      "\u001b[0mRobustScaler_SVM-PCA: 0.795181 (0.108223) [test_score:0.738]\n",
      "\u001b[0mQuantileTransformer-Normal_SVM-PCA: 0.734940 (0.086618) [test_score:0.762]\n",
      "\u001b[0mQuantileTransformer-Uniform_SVM-PCA: 0.789157 (0.099026) [test_score:0.738]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_SVM-PCA: 0.759036 (0.115976) [test_score:0.738]\n",
      "\u001b[0mNormalizer_SVM-PCA: 0.632530 (0.088912) [test_score:0.571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "C  = [x for x in np.arange(0.1, 2, 0.2)]\n",
    "kernel   = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'SVM__C': C,\n",
    "               'SVM__kernel': kernel,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_SVM' ,Pipeline([('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM',Pipeline([('Scaler',MinMaxScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM',Pipeline([('Scaler',MaxAbsScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM',Pipeline([('Scaler',RobustScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_SVM',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_SVM',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_SVM',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM',Pipeline([('Scaler',Normalizer()),('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_SVM-PCA',Pipeline([('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Normal_SVM-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_SVM-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_SVM-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "        \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LR: 0.777108 (0.085984)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LR: 0.753012 (0.103062)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LR: 0.783133 (0.111211)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.7s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LR: 0.789157 (0.111642)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LR: 0.783133 (0.110311)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   14.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_LR: 0.771084 (0.135003)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   16.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_LR: 0.813253 (0.116741)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   21.6s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_LR: 0.801205 (0.110630)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.7s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LR: 0.746988 (0.124564)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    0.8s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LR-PCA: 0.777108 (0.085984)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LR-PCA: 0.777108 (0.124496)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LR-PCA: 0.765060 (0.124579)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LR-PCA: 0.771084 (0.128618)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    2.5s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LR-PCA: 0.746988 (0.131147)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   25.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Normal_LR-PCA: 0.771084 (0.143357)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   22.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer-Uniform_LR-PCA: 0.801205 (0.102417)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   25.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerTransformer-yeo-johnson_LR-PCA: 0.783133 (0.120667)\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LR-PCA: 0.656627 (0.084885)\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_LR: 0.777108 (0.085984) [test_score:0.810]\n",
      "\u001b[0mScaled_LR: 0.753012 (0.103062) [test_score:0.762]\n",
      "\u001b[0mMinMax_LR: 0.783133 (0.111211) [test_score:0.762]\n",
      "\u001b[0mMaxAbsScaler_LR: 0.789157 (0.111642) [test_score:0.762]\n",
      "\u001b[0mRobustScaler_LR: 0.783133 (0.110311) [test_score:0.738]\n",
      "\u001b[0mQuantileTransformer-Normal_LR: 0.771084 (0.135003) [test_score:0.810]\n",
      "\u001b[1mQuantileTransformer-Uniform_LR: 0.813253 (0.116741) [test_score:0.762]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LR: 0.801205 (0.110630) [test_score:0.810]\n",
      "\u001b[0mNormalizer_LR: 0.746988 (0.124564) [test_score:0.690]\n",
      "\n",
      "\u001b[0m_LR-PCA: 0.777108 (0.085984) [test_score:0.810]\n",
      "\u001b[0mScaled_LR-PCA: 0.777108 (0.124496) [test_score:0.738]\n",
      "\u001b[0mMinMax_LR-PCA: 0.765060 (0.124579) [test_score:0.690]\n",
      "\u001b[0mMaxAbsScaler_LR-PCA: 0.771084 (0.128618) [test_score:0.714]\n",
      "\u001b[0mRobustScaler_LR-PCA: 0.746988 (0.131147) [test_score:0.714]\n",
      "\u001b[0mQuantileTransformer-Normal_LR-PCA: 0.771084 (0.143357) [test_score:0.714]\n",
      "\u001b[0mQuantileTransformer-Uniform_LR-PCA: 0.801205 (0.102417) [test_score:0.738]\n",
      "\u001b[0mPowerTransformer-yeo-johnson_LR-PCA: 0.783133 (0.120667) [test_score:0.714]\n",
      "\u001b[0mNormalizer_LR-PCA: 0.656627 (0.084885) [test_score:0.571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    1.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "C  = [x for x in np.arange(0.1, 3, 0.2)]\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "# dual = [True, False]\n",
    "fit_intercept = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'LR__C': C,\n",
    "               'LR__penalty': penalty,\n",
    "#                'LR__dual': dual,\n",
    "               'LR__fit_intercept': fit_intercept\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_LR',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR',Pipeline([('Scaler',MinMaxScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR',Pipeline([('Scaler',MaxAbsScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR',Pipeline([('Scaler',RobustScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LR',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LR',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LR',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')),('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR',Pipeline([('Scaler',Normalizer()),('LR',LogisticRegression())])))\n",
    "\n",
    "pipelines.append(('_LR-PCA',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Normal_LR-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='normal')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer-Uniform_LR-PCA',Pipeline([('Scaler',QuantileTransformer(output_distribution='uniform')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('PowerTransformer-yeo-johnson_LR-PCA',Pipeline([('Scaler',PowerTransformer(method='yeo-johnson')), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('LR',LogisticRegression())])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "        \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LDA: 0.789157 (0.133339)\n",
      "Test score 0.6904761904761905\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LDA: 0.777108 (0.091717)\n",
      "Test score 0.8095238095238095\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LDA: 0.777108 (0.095270)\n",
      "Test score 0.7380952380952381\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LDA: 0.777108 (0.095270)\n",
      "Test score 0.7380952380952381\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LDA: 0.777108 (0.092079)\n",
      "Test score 0.7857142857142857\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    3.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_LDA: 0.807229 (0.104187)\n",
      "Test score 0.8333333333333334\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_LDA: 0.746988 (0.121868)\n",
      "Test score 0.6904761904761905\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LDA-PCA: 0.789157 (0.133339)\n",
      "Test score 0.6904761904761905\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_LDA-PCA: 0.771084 (0.136876)\n",
      "Test score 0.7142857142857143\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_LDA-PCA: 0.771084 (0.128618)\n",
      "Test score 0.6666666666666666\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_LDA-PCA: 0.771084 (0.128618)\n",
      "Test score 0.6904761904761905\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_LDA-PCA: 0.740964 (0.122839)\n",
      "Test score 0.7142857142857143\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    3.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_LDA-PCA: 0.789157 (0.102542)\n",
      "Test score 0.7619047619047619\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n",
      "Normalizer_LDA-PCA: 0.656627 (0.086051)\n",
      "Test score 0.5952380952380952\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_LDA: 0.789157 (0.133339)\n",
      "\u001b[0mScaled_LDA: 0.777108 (0.091717)\n",
      "\u001b[0mMinMax_LDA: 0.777108 (0.095270)\n",
      "\u001b[0mMaxAbsScaler_LDA: 0.777108 (0.095270)\n",
      "\u001b[0mRobustScaler_LDA: 0.777108 (0.092079)\n",
      "\u001b[1mQuantileTransformer_LDA: 0.807229 (0.104187)\n",
      "\u001b[0mNormalizer_LDA: 0.746988 (0.121868)\n",
      "\n",
      "\u001b[0m_LDA-PCA: 0.789157 (0.133339)\n",
      "\u001b[0mScaled_LDA-PCA: 0.771084 (0.136876)\n",
      "\u001b[0mMinMax_LDA-PCA: 0.771084 (0.128618)\n",
      "\u001b[0mMaxAbsScaler_LDA-PCA: 0.771084 (0.128618)\n",
      "\u001b[0mRobustScaler_LDA-PCA: 0.740964 (0.122839)\n",
      "\u001b[0mQuantileTransformer_LDA-PCA: 0.789157 (0.102542)\n",
      "\u001b[0mNormalizer_LDA-PCA: 0.656627 (0.086051)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.1s finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "solver  = [\"lsqr\", \"eigen\"]\n",
    "shrinkage = [\"auto\",None, 0.1,0.3,0.5,0.7,0.9]\n",
    "# Create the random grid\n",
    "random_grid = {'LDA__solver': solver,\n",
    "               'LDA__shrinkage': shrinkage\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_LDA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA',Pipeline([('Scaler',MinMaxScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA',Pipeline([('Scaler',MaxAbsScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA',Pipeline([('Scaler',RobustScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer_LDA',Pipeline([('Scaler',QuantileTransformer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA',Pipeline([('Scaler',Normalizer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "pipelines.append(('_LDA-PCA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer_LDA-PCA',Pipeline([('Scaler',QuantileTransformer()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    \n",
    "    print(msg)\n",
    "        \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertune Multi Layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2484 candidates, totalling 24840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3922 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5136 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6514 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8052 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9754 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11616 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14162 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 17858 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 22558 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 24840 out of 24840 | elapsed:  8.0min finished\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_MLP: 0.849398 (0.066538)\n",
      "Fitting 10 folds for each of 2484 candidates, totalling 24840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3922 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5136 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6514 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8052 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9754 tasks      | elapsed:  3.6min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-204dd524598d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mbest_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import itertools\n",
    "\n",
    "hidden_layer_sizes = [(x,y) for x,y in itertools.product([x for x in range(1,3)],[x for x in range(5,120,5)])]\n",
    "activation = [ \"tanh\", \"relu\"]\n",
    "solver = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "alpha = [0.1,0.001,0.0001]\n",
    "learning_rate = [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "# Create the random grid\n",
    "random_grid = {'MLP__hidden_layer_sizes': hidden_layer_sizes,\n",
    "               'MLP__activation': activation,\n",
    "               'MLP__solver': solver,\n",
    "               'MLP__alpha': alpha,\n",
    "               'MLP__learning_rate': learning_rate,\n",
    "               'MLP__hidden_layer_sizes': hidden_layer_sizes,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_MLP',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP',Pipeline([('Scaler',StandardScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP',Pipeline([('Scaler',MinMaxScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP',Pipeline([('Scaler',MaxAbsScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP',Pipeline([('Scaler',RobustScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_MLP',Pipeline([('Scaler',QuantileTransformer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP',Pipeline([('Scaler',Normalizer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_MLP-PCA',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP-PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP-PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP-PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP-PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_MLP-PCA',Pipeline([('Scaler',QuantileTransformer()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP-PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=4)), ('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    \n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to C:\\Users\\User\\scikit_learn_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5082645 , 0.60330576, 0.6198347 , ..., 0.33471075, 0.3429752 ,\n",
       "        0.3429752 ],\n",
       "       [0.78099173, 0.7768595 , 0.77272725, ..., 0.1694215 , 0.1694215 ,\n",
       "        0.1694215 ],\n",
       "       [0.59504133, 0.661157  , 0.69008267, ..., 0.17355372, 0.20661157,\n",
       "        0.17355372],\n",
       "       ...,\n",
       "       [0.45454547, 0.3677686 , 0.23966943, ..., 0.446281  , 0.45041323,\n",
       "        0.45454547],\n",
       "       [0.14876033, 0.14876033, 0.14876033, ..., 0.4876033 , 0.46694216,\n",
       "        0.27272728],\n",
       "       [0.61157024, 0.72727275, 0.74380165, ..., 0.37190083, 0.47933885,\n",
       "        0.6694215 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_covtype, fetch_olivetti_faces\n",
    "\n",
    "fetched_dataset = fetch_olivetti_faces()\n",
    "# iris = load_iris()\n",
    "X, y = fetched_dataset.data, fetched_dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train as Dataframe\n",
    "# pd.DataFrame(X_train, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.965625 (0.025958)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.975000 (0.027243)\n",
      "KNN: 0.740625 (0.102746)\n",
      "CART: 0.468750 (0.069877)\n",
      "NB: 0.759375 (0.092755)\n",
      "SVM: 0.021875 (0.044305)\n",
      "RF: 0.246875 (0.088884)\n"
     ]
    }
   ],
   "source": [
    "num_folds=10\n",
    "scoring='accuracy'\n",
    "models = []\n",
    "models.append(('LR',  LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART',DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('NB',  GaussianNB()))\n",
    "models.append(('SVM', SVC(random_state=seed)))\n",
    "models.append(('RF',  RandomForestClassifier(max_depth=3, random_state=seed)))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: 0.934375 (0.066218)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\User\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLDA: 0.975000 (0.027243)\n",
      "ScaledKNN: 0.725000 (0.093541)\n",
      "ScaledCART: 0.475000 (0.075000)\n",
      "ScaledNB: 0.759375 (0.092755)\n",
      "ScaledSVM: 0.881250 (0.065252)\n",
      "ScaledRF: 0.246875 (0.088884)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART',Pipeline([('Scaler',StandardScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('ScaledNB',Pipeline([('Scaler',StandardScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('ScaledRF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No normalization or standartization\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "12.50%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.00%\n",
      "\n",
      "##################################################\n",
      "Data after standard scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.75%\n",
      "\n",
      "##################################################\n",
      "Data after min-max scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "12.81%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.00%\n",
      "\n",
      "##################################################\n",
      "Data after max-abs scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "12.50%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.00%\n",
      "\n",
      "##################################################\n",
      "Data after robust scaling\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.75%\n",
      "\n",
      "##################################################\n",
      "Data after power transformation (Yeo-Johnson)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.75%\n",
      "\n",
      "##################################################\n",
      "Data after quantile transformation (gaussian pdf)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "100.00%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "98.75%\n",
      "\n",
      "##################################################\n",
      "Data after quantile transformation (uniform pdf)\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "55.31%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "25.00%\n",
      "\n",
      "##################################################\n",
      "Data after sample-wise L2 normalizing\n",
      "\n",
      "Prediction accuracy for the training dataset\n",
      "12.50%\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.00%\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "\n",
    "# std_scale = StandardScaler().fit(X_train)\n",
    "distributions = [\n",
    "    ('Data after standard scaling',\n",
    "        StandardScaler()),\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler()),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler()),\n",
    "    ('Data after robust scaling',\n",
    "        RobustScaler(quantile_range=(25, 75))),\n",
    "    ('Data after power transformation (Yeo-Johnson)',\n",
    "     PowerTransformer(method='yeo-johnson')),\n",
    "#     ('Data after power transformation (Box-Cox)',\n",
    "#      PowerTransformer(method='box-cox')),\n",
    "    ('Data after quantile transformation (gaussian pdf)',\n",
    "        QuantileTransformer(output_distribution='normal')\n",
    "        ),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')\n",
    "        ),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer()),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(\"No normalization or standartization\")\n",
    "svc_scaled = SVC(C=1.5, random_state=seed)\n",
    "fit_std = svc_scaled.fit(X_train, y_train)\n",
    "pred_train_std = svc_scaled.predict(X_train)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "pred_test_std = svc_scaled.predict(X_test)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "\n",
    "\n",
    "print(\"#\"*50)\n",
    "\n",
    "    \n",
    "for name, dist in distributions:\n",
    "    print(name)\n",
    "    std_scale = dist.fit(X_train)\n",
    "    X_train_std = std_scale.transform(X_train)\n",
    "    X_test_std = std_scale.transform(X_test)\n",
    "\n",
    "    # on standardized data\n",
    "    svc_scaled = SVC(C=1.5, random_state=seed)\n",
    "    fit_std = svc_scaled.fit(X_train_std, y_train)\n",
    "    pred_train_std = svc_scaled.predict(X_train_std)\n",
    "\n",
    "    print('\\nPrediction accuracy for the training dataset')\n",
    "    print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_train_std)))\n",
    "    \n",
    "    pred_test_std = svc_scaled.predict(X_test_std)\n",
    "\n",
    "    print('\\nPrediction accuracy for the test dataset')\n",
    "    print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "    \n",
    "    \n",
    "    print(\"#\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f35f8845e5ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1303\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_LR',Pipeline([('LR',LogisticRegression())])))\n",
    "pipelines.append(('Scaled_LR',Pipeline([('Scaler',StandardScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MinMax_LR',Pipeline([('Scaler',MinMaxScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('MaxAbsScaler_LR',Pipeline([('Scaler',MaxAbsScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('RobustScaler_LR',Pipeline([('Scaler',RobustScaler()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('QuantileTransformer_LR',Pipeline([('Scaler',QuantileTransformer()),('LR',LogisticRegression())])))\n",
    "pipelines.append(('Normalizer_LR',Pipeline([('Scaler',Normalizer()),('LR',LogisticRegression())])))\n",
    "\n",
    "pipelines.append(('_LDA',Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Scaled_LDA',Pipeline([('Scaler',StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MinMax_LDA',Pipeline([('Scaler',MinMaxScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('MaxAbsScaler_LDA',Pipeline([('Scaler',MaxAbsScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('RobustScaler_LDA',Pipeline([('Scaler',RobustScaler()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('QuantileTransformer_LDA',Pipeline([('Scaler',QuantileTransformer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('Normalizer_LDA',Pipeline([('Scaler',Normalizer()),('LDA',LinearDiscriminantAnalysis())])))\n",
    "\n",
    "pipelines.append(('_KNN',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN',Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN',Pipeline([('Scaler',MaxAbsScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN',Pipeline([('Scaler',RobustScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer_KNN',Pipeline([('Scaler',QuantileTransformer()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN',Pipeline([('Scaler',Normalizer()),('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "pipelines.append(('_CART',Pipeline([('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_CART',Pipeline([('Scaler',StandardScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_CART',Pipeline([('Scaler',MinMaxScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_CART',Pipeline([('Scaler',MaxAbsScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_CART',Pipeline([('Scaler',RobustScaler()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_CART',Pipeline([('Scaler',QuantileTransformer()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_CART',Pipeline([('Scaler',Normalizer()),('CART',DecisionTreeClassifier(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_NB',Pipeline([('NB',GaussianNB())])))\n",
    "pipelines.append(('Scaled_NB',Pipeline([('Scaler',StandardScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('MinMax_NB',Pipeline([('Scaler',MinMaxScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('MaxAbsScaler_NB',Pipeline([('Scaler',MaxAbsScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('RobustScaler_NB',Pipeline([('Scaler',RobustScaler()),('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer_NB',Pipeline([('Scaler',QuantileTransformer()),('NB',GaussianNB())])))\n",
    "pipelines.append(('Normalizer_NB',Pipeline([('Scaler',Normalizer()),('NB',GaussianNB())])))\n",
    "\n",
    "pipelines.append(('_NB_PCA',Pipeline([('NB',GaussianNB())])))\n",
    "pipelines.append(('Scaled_NB_PCA',Pipeline([('Scaler',StandardScaler()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('MinMax_NB_PCA',Pipeline([('Scaler',MinMaxScaler()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('MaxAbsScaler_NB_PCA',Pipeline([('Scaler',MaxAbsScaler()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('RobustScaler_NB_PCA',Pipeline([('Scaler',RobustScaler()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('QuantileTransformer_NB_PCA',Pipeline([('Scaler',QuantileTransformer()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "pipelines.append(('Normalizer_NB_PCA',Pipeline([('Scaler',Normalizer()), ('PCA', PCA(n_components=2)), ('NB',GaussianNB())])))\n",
    "\n",
    "\n",
    "pipelines.append(('_SVM' ,Pipeline([('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM',Pipeline([('Scaler',MinMaxScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM',Pipeline([('Scaler',MaxAbsScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM',Pipeline([('Scaler',RobustScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_SVM',Pipeline([('Scaler',QuantileTransformer()), ('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM',Pipeline([('Scaler',Normalizer()), ('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_RF' ,Pipeline([('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF',Pipeline([('Scaler',MinMaxScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF',Pipeline([('Scaler',MaxAbsScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF',Pipeline([('Scaler',RobustScaler()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_RF',Pipeline([('Scaler',QuantileTransformer()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF',Pipeline([('Scaler',Normalizer()),('RF',RandomForestClassifier(max_depth=3, random_state=seed))])))\n",
    "\n",
    "pipelines.append(('_MLP',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP',Pipeline([('Scaler',StandardScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP',Pipeline([('Scaler',MinMaxScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP',Pipeline([('Scaler',MaxAbsScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP',Pipeline([('Scaler',RobustScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_MLP',Pipeline([('Scaler',QuantileTransformer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP',Pipeline([('Scaler',Normalizer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "print_results(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_LR: 0.965625 (0.025958)\n",
      "\u001b[0mScaled_LR: 0.934375 (0.066218)\n",
      "\u001b[0mMinMax_LR: 0.968750 (0.019764)\n",
      "\u001b[0mMaxAbsScaler_LR: 0.965625 (0.025958)\n",
      "\u001b[1mRobustScaler_LR: 0.978125 (0.020010)\n",
      "\u001b[0mQuantileTransformer_LR: 0.971875 (0.021875)\n",
      "\u001b[0mNormalizer_LR: 0.071875 (0.031406)\n",
      "\n",
      "\u001b[1m_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mScaled_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mMinMax_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mMaxAbsScaler_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mRobustScaler_LDA: 0.975000 (0.027243)\n",
      "\u001b[0mQuantileTransformer_LDA: 0.975000 (0.018750)\n",
      "\u001b[0mNormalizer_LDA: 0.956250 (0.031869)\n",
      "\n",
      "\u001b[1m_KNN: 0.740625 (0.102746)\n",
      "\u001b[0mScaled_KNN: 0.725000 (0.093541)\n",
      "\u001b[0mMinMax_KNN: 0.728125 (0.105558)\n",
      "\u001b[0mMaxAbsScaler_KNN: 0.725000 (0.091430)\n",
      "\u001b[0mRobustScaler_KNN: 0.734375 (0.079365)\n",
      "\u001b[0mQuantileTransformer_KNN: 0.725000 (0.100584)\n",
      "\u001b[0mNormalizer_KNN: 0.706250 (0.103833)\n",
      "\n",
      "\u001b[0m_CART: 0.468750 (0.069877)\n",
      "\u001b[0mScaled_CART: 0.475000 (0.075000)\n",
      "\u001b[0mMinMax_CART: 0.471875 (0.071875)\n",
      "\u001b[0mMaxAbsScaler_CART: 0.465625 (0.067676)\n",
      "\u001b[0mRobustScaler_CART: 0.471875 (0.071875)\n",
      "\u001b[1mQuantileTransformer_CART: 0.478125 (0.079119)\n",
      "\u001b[0mNormalizer_CART: 0.443750 (0.090355)\n",
      "\n",
      "\u001b[1m_NB: 0.759375 (0.092755)\n",
      "\u001b[0mScaled_NB: 0.759375 (0.092755)\n",
      "\u001b[0mMinMax_NB: 0.759375 (0.092755)\n",
      "\u001b[0mMaxAbsScaler_NB: 0.759375 (0.092755)\n",
      "\u001b[0mRobustScaler_NB: 0.759375 (0.092755)\n",
      "\u001b[0mQuantileTransformer_NB: 0.750000 (0.090571)\n",
      "\u001b[0mNormalizer_NB: 0.737500 (0.070156)\n",
      "\n",
      "\u001b[0m_SVM: 0.021875 (0.044305)\n",
      "\u001b[1mScaled_SVM: 0.881250 (0.065252)\n",
      "\u001b[0mMinMax_SVM: 0.031250 (0.041926)\n",
      "\u001b[0mMaxAbsScaler_SVM: 0.021875 (0.044305)\n",
      "\u001b[0mRobustScaler_SVM: 0.806250 (0.116760)\n",
      "\u001b[0mQuantileTransformer_SVM: 0.071875 (0.046456)\n",
      "\u001b[0mNormalizer_SVM: 0.018750 (0.040020)\n",
      "\n",
      "\u001b[1m_RF: 0.246875 (0.088884)\n",
      "\u001b[0mScaled_RF: 0.246875 (0.088884)\n",
      "\u001b[0mMinMax_RF: 0.243750 (0.093541)\n",
      "\u001b[0mMaxAbsScaler_RF: 0.243750 (0.093541)\n",
      "\u001b[0mRobustScaler_RF: 0.243750 (0.093541)\n",
      "\u001b[0mQuantileTransformer_RF: 0.240625 (0.100827)\n",
      "\u001b[0mNormalizer_RF: 0.237500 (0.065848)\n",
      "\n",
      "\u001b[0m_MLP: 0.637500 (0.248118)\n",
      "\u001b[0mScaled_MLP: 0.931250 (0.043750)\n",
      "\u001b[0mMinMax_MLP: 0.865625 (0.075325)\n",
      "\u001b[0mMaxAbsScaler_MLP: 0.462500 (0.203389)\n",
      "\u001b[0mRobustScaler_MLP: 0.925000 (0.048814)\n",
      "\u001b[1mQuantileTransformer_MLP: 0.937500 (0.050389)\n",
      "\u001b[0mNormalizer_MLP: 0.562500 (0.142522)\n"
     ]
    }
   ],
   "source": [
    "print_results(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   56.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_RF: 0.850000 (0.058962)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 660 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 943 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1308 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1753 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2280 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2887 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3576 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4345 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5196 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6127 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_RF: 0.853125 (0.059375)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2699 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3226 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3833 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4522 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5291 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6142 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_RF: 0.843750 (0.057622)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2849 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3376 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3983 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4672 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5441 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6292 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_RF: 0.850000 (0.058962)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed: 19.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_RF: 0.850000 (0.058962)\n",
      "Test score 0.925\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 31.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 44.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 60.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 79.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 100.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 124.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 150.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 178.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed: 198.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_RF: 0.825000 (0.050775)\n",
      "Test score 0.9\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 645 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 928 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1293 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1738 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2265 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2872 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3561 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4330 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5181 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6112 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_RF: 0.825000 (0.081729)\n",
      "Test score 0.875\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_RF: 0.850000 (0.058962)\n",
      "\u001b[1mScaled_RF: 0.853125 (0.059375)\n",
      "\u001b[0mMinMax_RF: 0.843750 (0.057622)\n",
      "\u001b[0mMaxAbsScaler_RF: 0.850000 (0.058962)\n",
      "\u001b[0mRobustScaler_RF: 0.850000 (0.058962)\n",
      "\u001b[0mQuantileTransformer_RF: 0.825000 (0.050775)\n",
      "\u001b[0mNormalizer_RF: 0.825000 (0.081729)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 20, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Create the random grid\n",
    "random_grid = {'RF__n_estimators': n_estimators,\n",
    "               'RF__max_features': max_features,\n",
    "               'RF__max_depth': max_depth,\n",
    "               'RF__min_samples_split': min_samples_split,\n",
    "               'RF__min_samples_leaf': min_samples_leaf,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_RF' ,Pipeline([('RF' , RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_RF' ,Pipeline([('Scaler',StandardScaler()),('RF' , RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_RF',Pipeline([('Scaler',MinMaxScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_RF',Pipeline([('Scaler',MaxAbsScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_RF',Pipeline([('Scaler',RobustScaler()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_RF',Pipeline([('Scaler',QuantileTransformer()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_RF',Pipeline([('Scaler',Normalizer()),('RF',RandomForestClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "\n",
    "for name, model in pipelines:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    print(\"Test score\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print_results2(names, results_mean,results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SVM: 0.962500 (0.033657)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_SVM: 0.953125 (0.037630)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_SVM: 0.956250 (0.034799)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_SVM: 0.962500 (0.033657)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_SVM: 0.959375 (0.034375)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 13.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_SVM: 0.959375 (0.039652)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   37.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_SVM: 0.028125 (0.038145)\n",
      "Test score 0.0\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[1m_SVM: 0.962500 (0.033657)\n",
      "\u001b[0mScaled_SVM: 0.953125 (0.037630)\n",
      "\u001b[0mMinMax_SVM: 0.956250 (0.034799)\n",
      "\u001b[1mMaxAbsScaler_SVM: 0.962500 (0.033657)\n",
      "\u001b[0mRobustScaler_SVM: 0.959375 (0.034375)\n",
      "\u001b[0mQuantileTransformer_SVM: 0.959375 (0.039652)\n",
      "\u001b[0mNormalizer_SVM: 0.028125 (0.038145)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "C  = [x for x in np.arange(0.1, 2, 0.2)]\n",
    "kernel   = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'SVM__C': C,\n",
    "               'SVM__kernel': kernel,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_SVM' ,Pipeline([('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('Scaled_SVM' ,Pipeline([('Scaler',StandardScaler()),('SVM' , SVC(random_state=seed))])))\n",
    "pipelines.append(('MinMax_SVM',Pipeline([('Scaler',MinMaxScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_SVM',Pipeline([('Scaler',MaxAbsScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_SVM',Pipeline([('Scaler',RobustScaler()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_SVM',Pipeline([('Scaler',QuantileTransformer()),('SVM',SVC(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_SVM',Pipeline([('Scaler',Normalizer()),('SVM',SVC(random_state=seed))])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    print(\"Test score\", clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   41.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_KNN: 0.921875 (0.048914)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   45.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_KNN: 0.925000 (0.046771)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   43.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax_KNN: 0.918750 (0.042390)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   44.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbsScaler_KNN: 0.918750 (0.046771)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler_KNN: 0.925000 (0.046771)\n",
      "Test score 0.9875\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer_KNN: 0.925000 (0.048814)\n",
      "Test score 0.9625\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   41.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer_KNN: 0.912500 (0.053765)\n",
      "Test score 0.9875\n",
      "\n",
      "##############################Results##############################\n",
      "\u001b[0m_KNN: 0.921875 (0.048914)\n",
      "\u001b[1mScaled_KNN: 0.925000 (0.046771)\n",
      "\u001b[0mMinMax_KNN: 0.918750 (0.042390)\n",
      "\u001b[0mMaxAbsScaler_KNN: 0.918750 (0.046771)\n",
      "\u001b[1mRobustScaler_KNN: 0.925000 (0.046771)\n",
      "\u001b[1mQuantileTransformer_KNN: 0.925000 (0.048814)\n",
      "\u001b[0mNormalizer_KNN: 0.912500 (0.053765)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "n_neighbors  = [int(x) for x in np.linspace(start = 1, stop = 20, num = 2)]\n",
    "weights  = [\"uniform\",\"distance\"]\n",
    "algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "leaf_size =[int(x) for x in np.linspace(start = 5, stop = 50, num = 2)]\n",
    "p =[int(x) for x in np.linspace(start = 1, stop = 4, num = 1)]\n",
    "# Create the random grid\n",
    "random_grid = {'KNN__n_neighbors': n_neighbors,\n",
    "               'KNN__weights': weights,\n",
    "               'KNN__algorithm': algorithm,\n",
    "               'KNN__leaf_size': leaf_size,\n",
    "               'KNN__p': p,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_KNN',Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Scaled_KNN',Pipeline([('Scaler',StandardScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MinMax_KNN',Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('MaxAbsScaler_KNN',Pipeline([('Scaler',MaxAbsScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('RobustScaler_KNN',Pipeline([('Scaler',RobustScaler()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('QuantileTransformer_KNN',Pipeline([('Scaler',QuantileTransformer()),('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('Normalizer_KNN',Pipeline([('Scaler',Normalizer()),('KNN',KNeighborsClassifier())])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "    print(\"Test score\", clf.score(X_test, y_test))\n",
    "\n",
    "print_results2(names, results_mean,results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2484 candidates, totalling 24840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 27.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 46.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 55.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 63.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 74.7min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-cac57eeec1b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mbest_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml1\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import itertools\n",
    "\n",
    "hidden_layer_sizes = [(x,y) for x,y in itertools.product([x for x in range(1,3)],[x for x in range(5,120,5)])]\n",
    "activation = [ \"tanh\", \"relu\"]\n",
    "solver = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "alpha = [0.1,0.001,0.0001]\n",
    "learning_rate = [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "# Create the random grid\n",
    "random_grid = {'MLP__hidden_layer_sizes': hidden_layer_sizes,\n",
    "               'MLP__activation': activation,\n",
    "               'MLP__solver': solver,\n",
    "               'MLP__alpha': alpha,\n",
    "               'MLP__learning_rate': learning_rate,\n",
    "               'MLP__hidden_layer_sizes': hidden_layer_sizes,\n",
    "              }\n",
    "\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('_MLP',Pipeline([('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Scaled_MLP',Pipeline([('Scaler',StandardScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MinMax_MLP',Pipeline([('Scaler',MinMaxScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('MaxAbsScaler_MLP',Pipeline([('Scaler',MaxAbsScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('RobustScaler_MLP',Pipeline([('Scaler',RobustScaler()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('QuantileTransformer_MLP',Pipeline([('Scaler',QuantileTransformer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "pipelines.append(('Normalizer_MLP',Pipeline([('Scaler',Normalizer()),('MLP',MLPClassifier(random_state=seed))])))\n",
    "\n",
    "\n",
    "results_mean = []\n",
    "results_std = []\n",
    "names = []\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "for name, model in pipelines:\n",
    "    clf = GridSearchCV(estimator = model, param_grid = random_grid, cv = kfold, verbose=2, n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_grid = clf.best_estimator_\n",
    "\n",
    "    best_mean = clf.cv_results_[\"mean_test_score\"][clf.best_index_]\n",
    "    best_std = clf.cv_results_[\"std_test_score\"][clf.best_index_]\n",
    "    results_mean.append(best_mean)\n",
    "    results_std.append(best_std)\n",
    "    names.append(name)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, best_mean, best_std)\n",
    "    print(msg)\n",
    "\n",
    "print_results2(names, results_mean,results_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
